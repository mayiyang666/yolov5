# Ultralytics YOLOv5 ğŸš€, AGPL-3.0 license
"""
Train a YOLOv5 model on a custom dataset. Models and datasets download automatically from the latest YOLOv5 release.

Usage - Single-GPU training:
    $ python train.py --data coco128.yaml --weights yolov5s.pt --img 640  # from pretrained (recommended)
    $ python train.py --data coco128.yaml --weights '' --cfg yolov5s.yaml --img 640  # from scratch

Usage - Multi-GPU DDP training:
    $ python -m torch.distributed.run --nproc_per_node 4 --master_port 1 train.py --data coco128.yaml --weights yolov5s.pt --img 640 --device 0,1,2,3

Models:     https://github.com/ultralytics/yolov5/tree/master/models
Datasets:   https://github.com/ultralytics/yolov5/tree/master/data
Tutorial:   https://docs.ultralytics.com/yolov5/tutorials/train_custom_data
"""
# Ultralytics YOLOv5 ğŸš€ï¼ŒAGPL-3.0 è®¸å¯åè®®
"""
åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šè®­ç»ƒ YOLOv5 æ¨¡å‹ã€‚æ¨¡å‹å’Œæ•°æ®é›†ä¼šè‡ªåŠ¨ä»æœ€æ–°çš„ YOLOv5 å‘å¸ƒç‰ˆæœ¬ä¸‹è½½ã€‚

ç”¨æ³• - å• GPU è®­ç»ƒï¼š
    $ python train.py --data coco128.yaml --weights yolov5s.pt --img 640  # ä»é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹ï¼ˆæ¨èï¼‰
    $ python train.py --data coco128.yaml --weights '' --cfg yolov5s.yaml --img 640  # ä»å¤´å¼€å§‹è®­ç»ƒ

ç”¨æ³• - å¤š GPU DDP è®­ç»ƒï¼š
    $ python -m torch.distributed.run --nproc_per_node 4 --master_port 1 train.py --data coco128.yaml --weights yolov5s.pt --img 640 --device 0,1,2,3

æ¨¡å‹ï¼š     https://github.com/ultralytics/yolov5/tree/master/models
æ•°æ®é›†ï¼š   https://github.com/ultralytics/yolov5/tree/master/data
æ•™ç¨‹ï¼š     https://docs.ultralytics.com/yolov5/tutorials/train_custom_data
"""

# æ ‡å‡†åº“æ¨¡å—
import argparse # å‚æ•°è§£æ
import math # æ•°å­¦è¿ç®—
import os # æ–‡ä»¶ç³»ç»Ÿæ“ä½œ
import random # éšæœºæ•°ç”Ÿæˆ
import subprocess # å­è¿›ç¨‹ç®¡ç†
import sys # Python è¿è¡Œç¯å¢ƒ
import time # æ—¶é—´æ“ä½œ
from copy import deepcopy # æ·±æ‹·è´
from datetime import datetime, timedelta # æ—¥æœŸä¸æ—¶é—´æ“ä½œ
from pathlib import Path # è·¯å¾„å¤„ç†
# ä¸€ä¸ªå®‰å…¨å¼•å…¥æ¨¡å—çš„æœºåˆ¶
# comet_ml æ˜¯ä¸€ä¸ªç”¨äºå®éªŒè¿½è¸ªå’Œæ—¥å¿—è®°å½•çš„å·¥å…·ï¼Œå¯ä»¥å¸®åŠ©å¼€å‘è€…è®°å½•æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°ã€æŒ‡æ ‡å’Œç»“æœã€‚
# å…¶å¼•å…¥éœ€è¦åœ¨ torch ä¹‹å‰å®Œæˆï¼Œå› ä¸º comet_ml å¯èƒ½ä¼šä¿®æ”¹ä¸€äº›åº•å±‚è¡Œä¸ºï¼Œå¦‚åŠ é€Ÿæ—¥å¿—è®°å½•çš„åŠŸèƒ½ã€‚
try:
    import comet_ml  # must be imported before torch (if installed) å¦‚æœæ¨¡å—å­˜åœ¨å¹¶æˆåŠŸå¼•å…¥ï¼Œåˆ™åç»­ä»£ç å¯ä»¥æ­£å¸¸ä½¿ç”¨å®ƒã€‚
except ImportError: # å¦‚æœæ¨¡å—ä¸å­˜åœ¨ï¼ˆImportErrorï¼‰ï¼Œå°† comet_ml å˜é‡è®¾ç½®ä¸º Noneï¼Œä»¥é¿å…è„šæœ¬æŠ¥é”™
    comet_ml = None

# ç¬¬ä¸‰æ–¹æ¨¡å—
import numpy as np
import torch
import torch.distributed as dist
import torch.nn as nn
import yaml # yaml æ–‡ä»¶å¤„ç†
from torch.optim import lr_scheduler
from tqdm import tqdm # è¿›åº¦æ¡æ˜¾ç¤º

# 1.ç¡®ä¿å½“å‰è„šæœ¬èƒ½å¤Ÿæ­£ç¡®æ‰¾åˆ° YOLOv5 é¡¹ç›®çš„æ ¹ç›®å½•ã€‚
# 2.é€šè¿‡å°†æ ¹ç›®å½•åŠ å…¥ sys.pathï¼Œä¿è¯ä»£ç èƒ½å¤Ÿå¯¼å…¥é¡¹ç›®ä¸­çš„æ¨¡å—ï¼Œå³ä½¿ç”¨æˆ·ä»ä»»æ„è·¯å¾„è¿è¡Œè¯¥è„šæœ¬ã€‚
# 3.è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„ï¼Œä½¿è°ƒè¯•è¾“å‡ºå’Œæ—¥å¿—è®°å½•æ›´å…·å¯è¯»æ€§ã€‚
FILE = Path(__file__).resolve() # __file__ æ˜¯å½“å‰è„šæœ¬çš„ç›¸å¯¹è·¯å¾„ï¼Œresolve() å°†å…¶è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ã€‚
ROOT = FILE.parents[0]  # YOLOv5 root directory è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„ç›®å½•è·¯å¾„ï¼Œå³å°†å½“å‰æ–‡ä»¶çš„è·¯å¾„å‘ä¸Šä¸€çº§è½¬æ¢ã€‚
if str(ROOT) not in sys.path: # æ£€æŸ¥ ROOT æ˜¯å¦å·²ç»åœ¨ Python çš„ sys.path ä¸­ã€‚ sys.path æ˜¯ Python ç”¨æ¥æœç´¢æ¨¡å—çš„è·¯å¾„åˆ—è¡¨ã€‚
    sys.path.append(str(ROOT))  # add ROOT to PATH å¦‚æœ ROOT ä¸åœ¨ sys.path ä¸­ï¼Œå°†å…¶åŠ å…¥ã€‚è¿™æ ·å¯ä»¥ç¡®ä¿è„šæœ¬èƒ½å¤Ÿä» ROOT ä¸­å¯¼å…¥æ¨¡å—ï¼ˆå¦‚ modelsã€utils ç­‰ï¼‰ã€‚
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative å°†ç»å¯¹è·¯å¾„ ROOT è½¬æ¢ä¸ºç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•çš„è·¯å¾„ã€‚

# å¯¼å…¥éªŒè¯æ¨¡å— valï¼Œå¹¶ç»™å®ƒä¸€ä¸ªåˆ«å validateã€‚
# é€šå¸¸ï¼ŒéªŒè¯æ¨¡å—ç”¨äºåœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸç»“æŸæ—¶è®¡ç®— mAPï¼ˆmean Average Precisionï¼‰ ç­‰è¯„ä¼°æŒ‡æ ‡ï¼ŒéªŒè¯æ¨¡å‹çš„è¡¨ç°ã€‚
import val as validate  # for end-of-epoch mAP
# attempt_loadï¼šç”¨äºåŠ è½½æ¨¡å‹æ–‡ä»¶ï¼ˆå¦‚ .pt æ ¼å¼çš„æƒé‡æ–‡ä»¶ï¼‰ã€‚
# experimental ä¸­çš„æ¨¡å—é€šå¸¸åŒ…å«ä¸€äº›å®éªŒæ€§çš„åŠŸèƒ½ï¼Œå¯èƒ½æ˜¯æœ€æ–°çš„æˆ–å°šæœªæ­£å¼ç¨³å®šçš„ç‰¹æ€§ã€‚
from models.experimental import attempt_load
# å¯¼å…¥ YOLOv5 çš„æ ¸å¿ƒæ¨¡å‹ç»“æ„ï¼Œç”¨äºå®šä¹‰å’Œè®­ç»ƒ YOLO ç½‘ç»œæ¨¡å‹ã€‚
from models.yolo import Model
# ç”¨äºæ£€æŸ¥å’Œä¼˜åŒ–é”šæ¡†ã€‚YOLO æ¨¡å‹çš„é”šæ¡†æ˜¯é¢„å®šä¹‰çš„è¾¹ç•Œæ¡†å°ºå¯¸ï¼Œç”¨äºé¢„æµ‹ç›®æ ‡çš„ä½ç½®ã€‚
from utils.autoanchor import check_anchors
# ç”¨äºæ£€æŸ¥å¹¶è°ƒæ•´è®­ç»ƒæ—¶çš„æ‰¹é‡å¤§å°ã€‚æ‰¹é‡å¤§å°ä¼šå½±å“è®­ç»ƒçš„é€Ÿåº¦å’Œå†…å­˜æ¶ˆè€—ï¼Œå› æ­¤éœ€è¦æ ¹æ®ç¡¬ä»¶æ¡ä»¶æ¥ä¼˜åŒ–ã€‚
from utils.autobatch import check_train_batch_size
# è¿™ä¸ªæ¨¡å—é€šå¸¸ç”¨äºå®ç°è®­ç»ƒè¿‡ç¨‹ä¸­çš„å›è°ƒå‡½æ•°ã€‚ä¾‹å¦‚ï¼Œè®­ç»ƒä¸­å¯ä»¥æ‰§è¡Œç‰¹å®šæ“ä½œï¼ˆå¦‚å­¦ä¹ ç‡è°ƒæ•´ã€æ¨¡å‹ä¿å­˜ç­‰ï¼‰å½“æ»¡è¶³æŸäº›æ¡ä»¶æ—¶ã€‚
from utils.callbacks import Callbacks
# ç”¨äºåˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼Œè¯»å–è®­ç»ƒã€éªŒè¯æ•°æ®ï¼Œå¹¶æ”¯æŒæ‰¹é‡åŠ è½½ã€‚
from utils.dataloaders import create_dataloader
# attempt_downloadï¼šç”¨äºå°è¯•ä¸‹è½½æ–‡ä»¶ï¼Œé€šå¸¸ç”¨äºä¸‹è½½é¢„è®­ç»ƒçš„æ¨¡å‹æˆ–æ•°æ®é›†ã€‚
# is_urlï¼šç”¨äºæ£€æŸ¥ç»™å®šçš„å­—ç¬¦ä¸²æ˜¯å¦ä¸º URLï¼Œé€šå¸¸ç”¨äºéªŒè¯ä¸‹è½½æºçš„æœ‰æ•ˆæ€§ã€‚
from utils.downloads import attempt_download, is_url
# å¯¼å…¥å¸¸è§å·¥å…·å‡½æ•°
from utils.general import (
    LOGGER,                     # LOGGER ç”¨äºè®°å½•æ—¥å¿—
    TQDM_BAR_FORMAT,            # TQDM_BAR_FORMAT ç”¨äºè°ƒæ•´è¿›åº¦æ¡çš„æ ¼å¼
    check_amp,                  # check_amp ç”¨äºæ£€æŸ¥è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰æ˜¯å¦æ”¯æŒ
    check_dataset,              # check_dataset ç”¨äºéªŒè¯æ•°æ®é›†çš„æ­£ç¡®æ€§
    check_file,
    check_git_info,             # check_git_info å’Œ check_git_status ç”¨äºè·å– Git ä»“åº“ä¿¡æ¯
    check_git_status,
    check_img_size,             # check_img_size ç”¨äºç¡®ä¿å›¾åƒå°ºå¯¸ç¬¦åˆè¦æ±‚
    check_requirements,
    check_suffix,
    check_yaml,
    colorstr,
    get_latest_run,
    increment_path,             # increment_path ç”¨äºåˆ›å»ºæ–°è·¯å¾„ä»¥é¿å…æ–‡ä»¶è¦†ç›–
    init_seeds,                 # init_seeds ç”¨äºè®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°æ€§
    intersect_dicts,            # intersect_dicts ç”¨äºå­—å…¸çš„äº¤é›†æ“ä½œ
    labels_to_class_weights,    # labels_to_class_weights å’Œ labels_to_image_weights ç”¨äºå¤„ç†æ ‡ç­¾æƒé‡
    labels_to_image_weights,
    methods,                    # methods å’Œ one_cycle é€šå¸¸ç”¨äºè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç®—æ³•ç­–ç•¥
    one_cycle,
    print_args,                 # print_args ç”¨äºæ‰“å°è®­ç»ƒé…ç½®
    print_mutation,
    strip_optimizer,            # strip_optimizer ç”¨äºä¼˜åŒ–å™¨ä¿®å‰ª
    yaml_save,                  # yaml_save ç”¨äºä¿å­˜é…ç½®
)
# LOGGERS å’Œ Loggersï¼šç”¨äºç®¡ç†å’Œè®°å½•è®­ç»ƒæ—¥å¿—ï¼Œå¯ä»¥æ”¯æŒä¸åŒçš„æ—¥å¿—è®°å½•å™¨ï¼ˆå¦‚æœ¬åœ°æ–‡ä»¶æ—¥å¿—ã€Comet.ml ç­‰ï¼‰ã€‚
from utils.loggers import LOGGERS, Loggers
# check_comet_resumeï¼šç”¨äºæ£€æŸ¥æ˜¯å¦å¯ä»¥ä» Comet.ml ä¸­æ¢å¤è®­ç»ƒï¼Œé€šå¸¸ç”¨äºæ”¯æŒè®­ç»ƒçš„ä¸­æ–­æ¢å¤ã€‚
from utils.loggers.comet.comet_utils import check_comet_resume
# ComputeLossï¼šè®¡ç®— YOLO æ¨¡å‹çš„æŸå¤±å‡½æ•°ã€‚è¯¥æŸå¤±å‡½æ•°ç»“åˆäº†ä½ç½®æŸå¤±ã€ç½®ä¿¡åº¦æŸå¤±ã€ç±»åˆ«æŸå¤±ç­‰ï¼Œç”¨äºæŒ‡å¯¼æ¨¡å‹ä¼˜åŒ–ã€‚
from utils.loss import ComputeLoss
# fitnessï¼šç”¨äºè®¡ç®—æ¨¡å‹çš„ fitnessï¼Œé€šå¸¸æ˜¯ä¸æ¨¡å‹æ€§èƒ½ç›¸å…³çš„åº¦é‡ï¼Œå¯èƒ½åŒ…æ‹¬å‡†ç¡®ç‡ã€mAP ç­‰ã€‚
from utils.metrics import fitness
# plot_evolveï¼šç”¨äºç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­æŒ‡æ ‡çš„æ¼”å˜ï¼ˆå¦‚æŸå¤±ã€mAP ç­‰ï¼‰ï¼Œä»¥å¸®åŠ©åˆ†ææ¨¡å‹è®­ç»ƒçš„è¿›å±•å’Œæ€§èƒ½ã€‚
from utils.plots import plot_evolve
# å¯¼å…¥ PyTorch ç›¸å…³æ¨¡å—
from utils.torch_utils import (
    EarlyStopping,      # æå‰åœæ­¢æœºåˆ¶ï¼Œå¦‚æœåœ¨è‹¥å¹²ä¸ªè®­ç»ƒå‘¨æœŸå†…æ²¡æœ‰æ˜¾è‘—æå‡æ€§èƒ½ï¼Œåˆ™æå‰ç»ˆæ­¢è®­ç»ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚
    ModelEMA,           # ç”¨äºæ¨¡å‹æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰çš„æ–¹æ³•ï¼Œå¸®åŠ©å¹³æ»‘æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ³¢åŠ¨ï¼Œæå‡æœ€ç»ˆæ¨¡å‹çš„ç¨³å®šæ€§ã€‚
    de_parallel,        # ç”¨äºå»é™¤å¤šå¡è®­ç»ƒæ—¶çš„æ¨¡å‹å¹¶è¡ŒåŒ–éƒ¨åˆ†ï¼Œæ¢å¤ä¸ºå•å¡æ¨¡å‹ã€‚
    select_device,      # é€‰æ‹©è®¡ç®—è®¾å¤‡ï¼ˆå¦‚ CPUã€GPUï¼‰ï¼Œæ ¹æ®ç¡¬ä»¶ç¯å¢ƒåŠ¨æ€é€‰æ‹©ã€‚
    smart_DDP,          # ç”¨äºæ™ºèƒ½åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œè®­ç»ƒï¼Œè‡ªåŠ¨é€‚é…å¤šå¡è®­ç»ƒç¯å¢ƒã€‚
    smart_optimizer,    # æ™ºèƒ½é€‰æ‹©ä¼˜åŒ–å™¨ï¼Œä¾æ®ä»»åŠ¡è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„ä¼˜åŒ–ç®—æ³•ã€‚
    smart_resume,       # ç”¨äºæ¢å¤ä¸­æ–­çš„è®­ç»ƒï¼Œè‡ªåŠ¨åŠ è½½æ–­ç‚¹æ–‡ä»¶ï¼Œæ¢å¤æ¨¡å‹è®­ç»ƒã€‚
    torch_distributed_zero_first, # ç”¨äºåˆ†å¸ƒå¼è®­ç»ƒï¼Œç¡®ä¿ rank 0 åœ¨è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒå‰èƒ½å¤Ÿé¡ºåˆ©åŠ è½½æ¨¡å‹å’Œæ•°æ®ã€‚
)

# åŠŸèƒ½ï¼šLOCAL_RANK æ˜¯åœ¨åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå°¤å…¶æ˜¯ä½¿ç”¨å¤šå¡è®­ç»ƒï¼‰ä¸­ï¼Œæ¯ä¸ªè¿›ç¨‹çš„æœ¬åœ°æ’åï¼ˆå³åœ¨å•ä¸ªèŠ‚ç‚¹ä¸Šçš„GPUç¼–å·ï¼‰ã€‚å®ƒä»ç¯å¢ƒå˜é‡ä¸­è·å–ï¼Œå¦‚æœç¯å¢ƒå˜é‡ä¸­æ²¡æœ‰è®¾ç½®ï¼Œé»˜è®¤ä¸º -1ã€‚
# ç”¨é€”ï¼šåœ¨å¤š GPU ç¯å¢ƒä¸‹ï¼ŒLOCAL_RANK ç”¨æ¥æ ‡è¯†å½“å‰è¿›ç¨‹ä½¿ç”¨çš„æ˜¯å“ªä¸€å— GPUã€‚åœ¨ PyTorch åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼ŒLOCAL_RANK ç»å¸¸ç”¨æ¥é€‰æ‹©ç‰¹å®šçš„è®¾å¤‡ï¼ˆå¦‚ GPUï¼‰ã€‚
LOCAL_RANK = int(os.getenv("LOCAL_RANK", -1))  # https://pytorch.org/docs/stable/elastic/run.html
# åŠŸèƒ½ï¼šRANK æ˜¯åœ¨æ•´ä¸ªåˆ†å¸ƒå¼è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå½“å‰è¿›ç¨‹çš„å…¨å±€æ’åï¼ˆå³åœ¨æ‰€æœ‰è¿›ç¨‹ä¸­çš„ç¼–å·ï¼‰ã€‚å®ƒä»ç¯å¢ƒå˜é‡ä¸­è·å–ï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®ï¼Œé»˜è®¤ä¸º -1ã€‚
# ç”¨é€”ï¼šRANK æ ‡è¯†äº†å½“å‰è¿›ç¨‹åœ¨æ‰€æœ‰è®­ç»ƒè¿›ç¨‹ä¸­çš„ä½ç½®ï¼Œé€šå¸¸ç”¨äºæ ‡è¯†æ¯ä¸ªè¿›ç¨‹çš„è§’è‰²ï¼ˆå¦‚ rank 0 é€šå¸¸ç”¨äºä¸»èŠ‚ç‚¹çš„ä»»åŠ¡ï¼Œå¦‚æ•°æ®åŠ è½½ã€ä¿å­˜æ¨¡å‹ç­‰ï¼‰ã€‚
#      åœ¨å¤šèŠ‚ç‚¹è®­ç»ƒä¸­ï¼ŒRANK ç”¨æ¥åŒºåˆ†ä¸åŒèŠ‚ç‚¹ä¹‹é—´çš„ä»»åŠ¡ã€‚
RANK = int(os.getenv("RANK", -1))
# åŠŸèƒ½ï¼šWORLD_SIZE æ˜¯åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„æ€»è¿›ç¨‹æ•°ï¼Œè¡¨ç¤ºå‚ä¸è®­ç»ƒçš„æ‰€æœ‰è¿›ç¨‹çš„æ•°é‡ã€‚å®ƒä»ç¯å¢ƒå˜é‡ä¸­è·å–ï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®ï¼Œé»˜è®¤ä¸º 1ã€‚
# ç”¨é€”ï¼šåœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼ŒWORLD_SIZE ç”¨æ¥çŸ¥é“æ€»å…±æœ‰å¤šå°‘ä¸ªè¿›ç¨‹å‚ä¸ã€‚å®ƒåœ¨åˆ†å¸ƒå¼é€šä¿¡ã€æ•°æ®åˆ†é…ã€æ¢¯åº¦åŒæ­¥ç­‰è¿‡ç¨‹ä¸­èµ·åˆ°äº†é‡è¦ä½œç”¨ã€‚
WORLD_SIZE = int(os.getenv("WORLD_SIZE", 1))
# åŠŸèƒ½ï¼šcheck_git_info() æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰çš„å‡½æ•°ï¼Œç”¨äºè·å–å½“å‰ä»£ç åº“çš„ Git ä¿¡æ¯ï¼Œå¦‚å½“å‰çš„æäº¤å“ˆå¸Œã€åˆ†æ”¯åç­‰ã€‚
# ç”¨é€”ï¼šé€šè¿‡ GIT_INFO å¯ä»¥è®°å½•å½“å‰æ¨¡å‹è®­ç»ƒæ—¶çš„ä»£ç ç‰ˆæœ¬ä¿¡æ¯ï¼Œç¡®ä¿æ¨¡å‹çš„å¯å¤ç°æ€§å’Œç‰ˆæœ¬è¿½æº¯ã€‚å¦‚æœå‡ºç°é—®é¢˜ï¼Œå¯ä»¥æ ¹æ® Git ä¿¡æ¯å›æº¯åˆ°å‡ºé”™çš„ä»£ç ç‰ˆæœ¬ã€‚
GIT_INFO = check_git_info()


def train(hyp, opt, device, callbacks):
    """
    Train a YOLOv5 model on a custom dataset using specified hyperparameters, options, and device, managing datasets,
    model architecture, loss computation, and optimizer steps.

    Args:
        hyp (str | dict): Path to the hyperparameters YAML file or a dictionary of hyperparameters.
        opt (argparse.Namespace): Parsed command-line arguments containing training options.
        device (torch.device): Device on which training occurs, e.g., 'cuda' or 'cpu'.
        callbacks (Callbacks): Callback functions for various training events.

    Returns:
        None

    Models and datasets download automatically from the latest YOLOv5 release.

    Example:
        Single-GPU training:
        ```bash
        $ python train.py --data coco128.yaml --weights yolov5s.pt --img 640  # from pretrained (recommended)
        $ python train.py --data coco128.yaml --weights '' --cfg yolov5s.yaml --img 640  # from scratch
        ```

        Multi-GPU DDP training:
        ```bash
        $ python -m torch.distributed.run --nproc_per_node 4 --master_port 1 train.py --data coco128.yaml --weights
        yolov5s.pt --img 640 --device 0,1,2,3
        ```

        For more usage details, refer to:
        - Models: https://github.com/ultralytics/yolov5/tree/master/models
        - Datasets: https://github.com/ultralytics/yolov5/tree/master/data
        - Tutorial: https://docs.ultralytics.com/yolov5/tutorials/train_custom_data
    """
    """
    ä½¿ç”¨æŒ‡å®šçš„è¶…å‚æ•°ã€é€‰é¡¹å’Œè®¾å¤‡ï¼Œåœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ª YOLOv5 æ¨¡å‹ï¼Œç®¡ç†æ•°æ®é›†ã€æ¨¡å‹æ¶æ„ã€æŸå¤±è®¡ç®—å’Œä¼˜åŒ–å™¨æ­¥éª¤ã€‚

    å‚æ•°ï¼š
        hyp (str | dict): è¶…å‚æ•° YAML æ–‡ä»¶çš„è·¯å¾„ï¼Œæˆ–è¶…å‚æ•°çš„å­—å…¸ã€‚
        opt (argparse.Namespace): è§£æåçš„å‘½ä»¤è¡Œå‚æ•°ï¼ŒåŒ…å«è®­ç»ƒé€‰é¡¹ã€‚
        device (torch.device): è®­ç»ƒæ‰€åœ¨çš„è®¾å¤‡ï¼Œä¾‹å¦‚ 'cuda' æˆ– 'cpu'ã€‚
        callbacks (Callbacks): ç”¨äºå„ç§è®­ç»ƒäº‹ä»¶çš„å›è°ƒå‡½æ•°ã€‚

    è¿”å›ï¼š
        None

    æ¨¡å‹å’Œæ•°æ®é›†ä¼šè‡ªåŠ¨ä»æœ€æ–°çš„ YOLOv5 å‘å¸ƒç‰ˆæœ¬ä¸‹è½½ã€‚

    ç¤ºä¾‹ï¼š
        å•GPUè®­ç»ƒï¼š
        ```bash
        $ python train.py --data coco128.yaml --weights yolov5s.pt --img 640  # ä»é¢„è®­ç»ƒå¼€å§‹ï¼ˆæ¨èï¼‰
        $ python train.py --data coco128.yaml --weights '' --cfg yolov5s.yaml --img 640  # ä»å¤´å¼€å§‹è®­ç»ƒ
        ```

        å¤šGPU DDPè®­ç»ƒï¼š
        ```bash
        $ python -m torch.distributed.run --nproc_per_node 4 --master_port 1 train.py --data coco128.yaml --weights
        yolov5s.pt --img 640 --device 0,1,2,3
        ```

        æ›´å¤šä½¿ç”¨è¯¦æƒ…ï¼Œè¯·å‚è€ƒï¼š
        - æ¨¡å‹: https://github.com/ultralytics/yolov5/tree/master/models
        - æ•°æ®é›†: https://github.com/ultralytics/yolov5/tree/master/data
        - æ•™ç¨‹: https://docs.ultralytics.com/yolov5/tutorials/train_custom_data
    """

    # ä»å‘½ä»¤è¡Œå‚æ•° opt ä¸­æå–å„ä¸ªè®­ç»ƒç›¸å…³çš„é…ç½®é€‰é¡¹ï¼Œå¹¶å°†å®ƒä»¬èµ‹å€¼ç»™å˜é‡
    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze = (
        Path(opt.save_dir), # ä¿å­˜æ¨¡å‹çš„ç›®å½•è·¯å¾„ã€‚
        opt.epochs,         # è®­ç»ƒçš„è½®æ•°ã€‚
        opt.batch_size,     # æ¯ä¸ªæ‰¹æ¬¡çš„å¤§å°ã€‚
        opt.weights,        # æƒé‡æ–‡ä»¶è·¯å¾„ï¼Œé€šå¸¸æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ã€‚
        opt.single_cls,     # æ˜¯å¦å°†æ‰€æœ‰ç±»åˆ«åˆå¹¶ä¸ºå•ä¸€ç±»åˆ«è®­ç»ƒã€‚
        opt.evolve,         # æ˜¯å¦å¯ç”¨è¶…å‚æ•°è¿›åŒ–ï¼ˆä¾‹å¦‚ï¼Œè‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡ç­‰ï¼‰ã€‚
        opt.data,           # æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„ã€‚
        opt.cfg,            # æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„ã€‚
        opt.resume,         # æ˜¯å¦ä»ä¸Šæ¬¡è®­ç»ƒä¸­æ–­çš„åœ°æ–¹æ¢å¤è®­ç»ƒã€‚
        opt.noval,          # æ˜¯å¦è·³è¿‡éªŒè¯ã€‚
        opt.nosave,         # æ˜¯å¦è·³è¿‡ä¿å­˜æ¨¡å‹ã€‚
        opt.workers,        # ç”¨äºæ•°æ®åŠ è½½çš„å·¥ä½œçº¿ç¨‹æ•°ã€‚
        opt.freeze,         # æ˜¯å¦å†»ç»“ç½‘ç»œçš„éƒ¨åˆ†å±‚è¿›è¡Œè®­ç»ƒã€‚
    )
    # è°ƒç”¨ callbacks.run("on_pretrain_routine_start") æ¥è§¦å‘é¢„è®­ç»ƒå¼€å§‹å‰çš„å›è°ƒå‡½æ•°
    callbacks.run("on_pretrain_routine_start")

    # Directories
    # æ ¹æ®æŒ‡å®šçš„ save_dir ç›®å½•è·¯å¾„åˆ›å»ºå­˜å‚¨æ¨¡å‹æƒé‡çš„å­ç›®å½•ï¼Œ
    # å¹¶å®šä¹‰ last.pt å’Œ best.pt æ–‡ä»¶è·¯å¾„ï¼Œåˆ†åˆ«ç”¨äºå­˜å‚¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ€åçš„æ¨¡å‹æƒé‡å’Œæœ€å¥½çš„æ¨¡å‹æƒé‡ã€‚
    w = save_dir / "weights"  # weights dir
    # åˆ›å»ºæƒé‡æ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨çš„è¯ï¼‰
    # w.parent: è¿™æ˜¯ w çš„çˆ¶ç›®å½•ï¼Œä¹Ÿå°±æ˜¯ save_dirã€‚å¦‚æœ evolve ä¸º Trueï¼Œåˆ™ä½¿ç”¨çˆ¶ç›®å½•æ¥å­˜å‚¨æ–‡ä»¶ã€‚
    # evolve: è¿™ä¸ªå˜é‡å†³å®šæ˜¯å¦å¯ç”¨è¶…å‚æ•°è¿›åŒ–ï¼ˆä¾‹å¦‚ï¼ŒåŠ¨æ€è°ƒæ•´æ¨¡å‹çš„è®­ç»ƒå‚æ•°ï¼‰ã€‚å¦‚æœå¯ç”¨ evolveï¼Œåˆ™ä¼šä½¿ç”¨ w.parentï¼ˆçˆ¶ç›®å½•ï¼‰æ¥å­˜å‚¨æ–‡ä»¶ã€‚
    # mkdir(parents=True, exist_ok=True): åˆ›å»ºç›®å½•æ—¶ï¼Œparents=True è¡¨ç¤ºå¦‚æœçˆ¶ç›®å½•ä¸å­˜åœ¨ä¹Ÿä¼šåˆ›å»ºï¼Œexist_ok=True è¡¨ç¤ºå¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œä¸ä¼šæŠ¥é”™ã€‚
    (w.parent if evolve else w).mkdir(parents=True, exist_ok=True)  # make dir
    # last: ç”¨äºå­˜å‚¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ€åä¸€ä¸ªä¿å­˜çš„æ¨¡å‹æƒé‡æ–‡ä»¶çš„è·¯å¾„ï¼Œæ–‡ä»¶åä¸º last.ptã€‚
    # best: ç”¨äºå­˜å‚¨è®­ç»ƒè¿‡ç¨‹ä¸­æ€§èƒ½æœ€å¥½çš„æ¨¡å‹æƒé‡æ–‡ä»¶çš„è·¯å¾„ï¼Œæ–‡ä»¶åä¸º best.ptã€‚
    last, best = w / "last.pt", w / "best.pt"

    # Hyperparameters
    # åŠ è½½è¶…å‚æ•°ï¼Œå¹¶è¾“å‡ºå®ƒä»¬çš„å€¼ï¼Œ
    # åŒæ—¶å°†è¶…å‚æ•°å­˜å‚¨åœ¨ opt å¯¹è±¡ä¸­ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ£€æŸ¥ç‚¹ï¼ˆcheckpointsï¼‰ä¸­è¿›è¡Œä¿å­˜
    if isinstance(hyp, str):
        # errors="ignore" å‚æ•°ç¡®ä¿å¦‚æœæ–‡ä»¶ç¼–ç å‡ºç°é—®é¢˜æ—¶ï¼Œä¸ä¼šæŠ›å‡ºé”™è¯¯ï¼Œè€Œæ˜¯å¿½ç•¥é”™è¯¯å¹¶ç»§ç»­æ‰§è¡Œã€‚
        with open(hyp, errors="ignore") as f:
            # yaml.safe_load(f) ä¼šå°† YAML æ–‡ä»¶å†…å®¹è½¬æ¢ä¸º Python å­—å…¸æ ¼å¼
            hyp = yaml.safe_load(f)  # load hyps dict
    # LOGGER.info(...) ç”¨äºè¾“å‡ºè¶…å‚æ•°çš„æ—¥å¿—ã€‚
    # colorstr("hyperparameters: "):
    #   è¿™éƒ¨åˆ†æ˜¯ä¸€ä¸ªå¸¦é¢œè‰²çš„æ—¥å¿—è¾“å‡ºï¼Œå…¶ä¸­ "hyperparameters: " ä¼šä»¥ç‰¹å®šçš„é¢œè‰²æ˜¾ç¤ºã€‚
    #   colorstr æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºä¸ºæ—¥å¿—æ–‡æœ¬æ·»åŠ é¢œè‰²ï¼Œé€šå¸¸ç”¨äºå¢å¼ºè¾“å‡ºçš„å¯è¯»æ€§ã€‚
    # ", ".join(f"{k}={v}" for k, v in hyp.items())ï¼š
    #   å°†è¶…å‚æ•°å­—å…¸ä¸­çš„æ¯ä¸ªé”®å€¼å¯¹æ ¼å¼åŒ–æˆ "key=value" çš„å½¢å¼ï¼Œå¹¶ç”¨é€—å·åˆ†éš”ï¼Œå½¢æˆä¸€ä¸ªå­—ç¬¦ä¸²ã€‚
    #   ä¾‹å¦‚ï¼šlearning_rate=0.001, batch_size=32ã€‚
    LOGGER.info(colorstr("hyperparameters: ") + ", ".join(f"{k}={v}" for k, v in hyp.items()))
    # å°†è¶…å‚æ•°å­—å…¸ hyp çš„å‰¯æœ¬èµ‹å€¼ç»™ opt.hypã€‚
    # è¿™æ ·åšçš„ç›®çš„æ˜¯ç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¶…å‚æ•°å¯ä»¥ä¿å­˜åœ¨æ¨¡å‹çš„æ£€æŸ¥ç‚¹ä¸­ï¼Œä»¥ä¾¿åœ¨æ¢å¤è®­ç»ƒæ—¶å¯ä»¥æŸ¥çœ‹æˆ–ä½¿ç”¨è¿™äº›è¶…å‚æ•°ã€‚
    # hyp.copy() æ˜¯ä¸ºäº†é¿å…åç»­å¯¹ hyp å­—å…¸çš„ä¿®æ”¹å½±å“åˆ° opt.hypï¼Œç¡®ä¿ opt.hyp ä¿å­˜çš„æ˜¯å½“æ—¶åŠ è½½çš„è¶…å‚æ•°å‰¯æœ¬ã€‚
    opt.hyp = hyp.copy()  # for saving hyps to checkpoints

    # Save run settings
    # åªæœ‰å½“ evolve ä¸º False æ—¶ï¼Œæ‰ä¼šæ‰§è¡Œä¿å­˜æ“ä½œã€‚
    # å¦åˆ™ï¼Œå¦‚æœå¯ç”¨äº†è¶…å‚æ•°è¿›åŒ–ï¼Œå¯èƒ½ä¼šè·³è¿‡ä¿å­˜è®¾ç½®çš„æ­¥éª¤ï¼Œå› ä¸ºè¿›åŒ–è¿‡ç¨‹å¯èƒ½ä¼šåœ¨å¤šæ¬¡è®­ç»ƒä¸­é€æ­¥è°ƒæ•´è¶…å‚æ•°ã€‚
    if not evolve:
        # yaml_save(save_dir / "hyp.yaml", hyp) å°†è¶…å‚æ•°å­—å…¸ hyp ä¿å­˜ä¸º hyp.yaml æ–‡ä»¶ã€‚
        # yaml_save æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰å‡½æ•°ï¼Œé€šå¸¸ä¼šè°ƒç”¨ yaml.dump() æ¥å°†å­—å…¸æ•°æ®æ ¼å¼åŒ–ä¸º YAML æ ¼å¼å¹¶å†™å…¥æ–‡ä»¶ã€‚
        yaml_save(save_dir / "hyp.yaml", hyp)
        # vars(opt) æ˜¯ä¸€ä¸ªå†…ç½®å‡½æ•°ï¼Œå®ƒè¿”å›ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«äº† opt å¯¹è±¡çš„æ‰€æœ‰å®ä¾‹å˜é‡ï¼ˆå³è®­ç»ƒçš„å‘½ä»¤è¡Œå‚æ•°å’Œé€‰é¡¹ï¼‰ã€‚
        # è¿™äº›é€‰é¡¹åŒ…æ‹¬æ‰¹å¤§å°ã€å­¦ä¹ ç‡ã€æ•°æ®é›†è·¯å¾„ç­‰å‚æ•°ã€‚
        yaml_save(save_dir / "opt.yaml", vars(opt))

    # Loggers
    data_dict = None
    # RANK å˜é‡é€šå¸¸ä¸åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¦‚å¤šGPUè®­ç»ƒï¼‰ç›¸å…³ã€‚
    # RANK ä¸º -1 æˆ– 0 è¡¨ç¤ºè¿™æ˜¯ä¸»è¿›ç¨‹ï¼ˆé€šå¸¸æ˜¯è¿›è¡Œæ—¥å¿—è®°å½•å’Œæ§åˆ¶çš„èŠ‚ç‚¹ï¼‰ã€‚
    # å¦‚æœåœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­åªæœ‰ä¸»è¿›ç¨‹ä¼šè®°å½•æ—¥å¿—ã€‚
    if RANK in {-1, 0}:
        # LOGGERS æ˜¯ä¸€ä¸ªå®šä¹‰äº†å¯ç”¨æ—¥å¿—è®°å½•å™¨çš„é›†åˆ
        include_loggers = list(LOGGERS)
        # å¦‚æœ opt.ndjson_console ä¸º Trueï¼Œåˆ™å°† ndjson_console æ·»åŠ åˆ°æ—¥å¿—è®°å½•å™¨åˆ—è¡¨ä¸­ã€‚
        if getattr(opt, "ndjson_console", False):
            include_loggers.append("ndjson_console")
        # å¦‚æœ opt.ndjson_file ä¸º Trueï¼Œåˆ™å°† ndjson_file æ·»åŠ åˆ°æ—¥å¿—è®°å½•å™¨åˆ—è¡¨ä¸­ã€‚
        if getattr(opt, "ndjson_file", False):
            include_loggers.append("ndjson_file")
        # åˆ›å»ºä¸€ä¸ª Loggers å¯¹è±¡ï¼Œä¼ å…¥è®­ç»ƒçš„ç›¸å…³å‚æ•°ï¼š
        loggers = Loggers(
            save_dir=save_dir,
            weights=weights,
            opt=opt,
            hyp=hyp,
            logger=LOGGER,
            include=tuple(include_loggers),
        )

        # Register actions
        # methods(loggers) è·å– loggers å¯¹è±¡ä¸­å¯ç”¨çš„æ–¹æ³•åˆ—è¡¨ï¼ˆä¾‹å¦‚ï¼Œä¿å­˜æ¨¡å‹ã€è®°å½•æŒ‡æ ‡ç­‰ï¼‰ã€‚
        # callbacks.register_action(k, callback=getattr(loggers, k))
        # å°†è¿™äº›æ–¹æ³•ä½œä¸ºå›è°ƒæ³¨å†Œåˆ° callbacks ä¸­ï¼Œç¡®ä¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€‚å½“æ—¶æœºè°ƒç”¨æ—¥å¿—è®°å½•æ–¹æ³•ã€‚
        for k in methods(loggers):
            callbacks.register_action(k, callback=getattr(loggers, k))

        # Process custom dataset artifact link
        # loggers.remote_dataset å¯èƒ½æ˜¯ä¸€ä¸ªä»è¿œç¨‹å­˜å‚¨æˆ–äº‘å¹³å°ä¸‹è½½çš„æ•°æ®é›†çš„é“¾æ¥æˆ–è·¯å¾„ä¿¡æ¯ã€‚
        # å¦‚æœé…ç½®äº†è¿œç¨‹æ•°æ®é›†é“¾æ¥ï¼Œdata_dict å°†åŒ…å«ç›¸å…³ä¿¡æ¯ã€‚
        data_dict = loggers.remote_dataset
        # å¦‚æœ resume ä¸º Trueï¼Œåˆ™æ„å‘³ç€æ­£åœ¨æ¢å¤ä¹‹å‰çš„è®­ç»ƒ
        if resume:  # If resuming runs from remote artifact
            # åœ¨æ¢å¤æ—¶ï¼Œé‡æ–°åŠ è½½æƒé‡ã€è®­ç»ƒè½®æ•°ã€è¶…å‚æ•°å’Œæ‰¹é‡å¤§å°ç­‰ä¿¡æ¯ï¼Œç¡®ä¿æ¢å¤çš„è®­ç»ƒçŠ¶æ€å’Œä¹‹å‰çš„è®­ç»ƒä¸€è‡´
            weights, epochs, hyp, batch_size = opt.weights, opt.epochs, opt.hyp, opt.batch_size

    # Config
    # plots å˜é‡å†³å®šæ˜¯å¦åˆ›å»ºå¯è§†åŒ–å›¾è¡¨ã€‚
    # evolve ä¸º True æ—¶ï¼Œé€šå¸¸è¡¨ç¤ºæ­£åœ¨è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–è¿‡ç¨‹ï¼Œè¿™æ—¶ä¸éœ€è¦ç”Ÿæˆè®­ç»ƒå›¾è¡¨ã€‚
    # opt.noplots å¦‚æœä¸º Trueï¼Œä¹Ÿè¡¨ç¤ºç¦ç”¨ç»˜å›¾ã€‚
    plots = not evolve and not opt.noplots  # create plots
    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ GPUï¼ˆCUDAï¼‰ã€‚å¦‚æœ device.type ä¸ç­‰äº "cpu"ï¼Œè¯´æ˜æ­£åœ¨ä½¿ç”¨ GPUã€‚
    cuda = device.type != "cpu"
    # init_seeds ç”¨äºåˆå§‹åŒ–éšæœºç§å­ï¼Œç¡®ä¿å®éªŒçš„å¯å¤ç°æ€§ã€‚
    # opt.seed æ˜¯ç”¨æˆ·æŒ‡å®šçš„éšæœºç§å­ï¼ŒRANK æ˜¯å½“å‰è®­ç»ƒè¿›ç¨‹çš„ç¼–å·ï¼Œ+1 å’Œ + RANK ç¡®ä¿æ¯ä¸ªè¿›ç¨‹çš„ç§å­ä¸åŒã€‚
    # deterministic=True è¡¨ç¤ºå¯ç”¨ç¡®å®šæ€§è®¡ç®—ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨å›ºå®šçš„éšæœºæ•°ç”Ÿæˆåºåˆ—ã€‚
    init_seeds(opt.seed + 1 + RANK, deterministic=True)
    # è¯¥ä»£ç å—ç¡®ä¿åœ¨åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒä¸­ï¼Œåªæœ‰ä¸»è¿›ç¨‹ï¼ˆRANK == 0ï¼‰ä¼šæ‰§è¡Œæ•°æ®é›†æ£€æŸ¥ã€‚
    # check_dataset(data) ä¼šæ£€æŸ¥å¹¶åŠ è½½æ•°æ®é›†ã€‚å¦‚æœ data_dict å·²ç»å­˜åœ¨ï¼Œåˆ™è·³è¿‡ã€‚
    # torch_distributed_zero_first(LOCAL_RANK) ç¡®ä¿åªåœ¨ä¸»è¿›ç¨‹ï¼ˆRANK == 0ï¼‰è¿›è¡Œæ“ä½œï¼Œé¿å…åœ¨å…¶ä»–è¿›ç¨‹ä¸­é‡å¤å·¥ä½œã€‚
    with torch_distributed_zero_first(LOCAL_RANK):
        data_dict = data_dict or check_dataset(data)  # check if None
    # data_dict æ˜¯é€šè¿‡ check_dataset(data) åŠ è½½çš„ï¼ŒåŒ…å«äº†è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„è·¯å¾„ä¿¡æ¯ã€‚
    train_path, val_path = data_dict["train"], data_dict["val"]
    # nc è¡¨ç¤ºç±»åˆ«æ•°ï¼ˆnumber of classesï¼‰ã€‚
    # å¦‚æœ single_cls ä¸º Trueï¼Œåˆ™è¡¨ç¤ºåªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œnc è®¾ç½®ä¸º 1ã€‚
    # å¦åˆ™ï¼Œnc ä»æ•°æ®å­—å…¸ä¸­è·å–ï¼Œè¡¨ç¤ºæ•°æ®é›†ä¸­çš„ç±»åˆ«æ•°é‡ã€‚
    nc = 1 if single_cls else int(data_dict["nc"])  # number of classes
    # å¦‚æœæ˜¯å•ç±»ä»»åŠ¡ï¼ˆsingle_cls=Trueï¼‰ï¼Œå¹¶ä¸”æ•°æ®å­—å…¸ä¸­çš„ names åˆ—è¡¨é•¿åº¦ä¸æ˜¯ 1ï¼Œåˆ™å°†ç±»åˆ«åç§°è®¾ç½®ä¸º {0: "item"}ï¼Œè¡¨ç¤ºå”¯ä¸€ç±»åˆ«ä¸º "item"ã€‚
    # å¦åˆ™ï¼Œä½¿ç”¨æ•°æ®å­—å…¸ä¸­çš„ namesï¼ˆå³å¤šä¸ªç±»åˆ«çš„åç§°ï¼‰ã€‚
    names = {0: "item"} if single_cls and len(data_dict["names"]) != 1 else data_dict["names"]  # class names
    # is_coco æ˜¯ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œç”¨äºæ£€æŸ¥éªŒè¯é›†è·¯å¾„æ˜¯å¦ä¸º COCO æ•°æ®é›†çš„è·¯å¾„ã€‚
    # å¦‚æœ val_path æ˜¯å­—ç¬¦ä¸²å¹¶ä¸”ä»¥ coco/val2017.txt ç»“å°¾ï¼Œè¯´æ˜æ˜¯ COCO æ•°æ®é›†ã€‚
    is_coco = isinstance(val_path, str) and val_path.endswith("coco/val2017.txt")  # COCO dataset

    # Model
    # check_suffix æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œç”¨æ¥æ£€æŸ¥ weights è·¯å¾„æ˜¯å¦ä»¥ .pt ç»“å°¾ï¼Œ
    # ç¡®ä¿æä¾›çš„æ˜¯ä¸€ä¸ª PyTorch æ¨¡å‹æƒé‡æ–‡ä»¶ï¼ˆé€šå¸¸ä¸º .pt æ ¼å¼ï¼‰
    check_suffix(weights, ".pt")  # check weights
    # å¦‚æœ weights æ˜¯ä»¥ .pt ç»“å°¾ï¼Œè¡¨ç¤ºæ˜¯é¢„è®­ç»ƒæ¨¡å‹
    pretrained = weights.endswith(".pt")
    if pretrained:
        # torch_distributed_zero_first(LOCAL_RANK) ç¡®ä¿åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­åªæœ‰ä¸»è¿›ç¨‹ä¸‹è½½æ¨¡å‹æƒé‡æ–‡ä»¶ã€‚
        with torch_distributed_zero_first(LOCAL_RANK):
            # attempt_download(weights) å°è¯•ä»è¿œç¨‹ä¸‹è½½ weights æ–‡ä»¶ï¼ˆå¦‚æœåœ¨æœ¬åœ°æ‰¾ä¸åˆ°çš„è¯ï¼‰
            weights = attempt_download(weights)  # download if not found locally
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„ checkpointï¼ˆæƒé‡æ–‡ä»¶ï¼‰ï¼Œå¹¶å°†å…¶åŠ è½½åˆ° CPU ä¸Šï¼Œé¿å…ç›´æ¥åŠ è½½åˆ° GPU æ—¶å¯èƒ½çš„ CUDA å†…å­˜æ³„æ¼ã€‚
        ckpt = torch.load(weights, map_location="cpu")  # load checkpoint to CPU to avoid CUDA memory leak
        # æ ¹æ®é…ç½®æ–‡ä»¶æˆ–åŠ è½½çš„ checkpoint åˆ›å»ºæ¨¡å‹å®ä¾‹ã€‚
        # cfg or ckpt["model"].yamlï¼šå¦‚æœ cfg å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨å®ƒä½œä¸ºé…ç½®æ–‡ä»¶ï¼›å¦åˆ™ï¼Œä½¿ç”¨ checkpoint ä¸­çš„é…ç½®ã€‚
        # ch=3 è¡¨ç¤ºè¾“å…¥é€šé“æ•°ï¼Œè¿™é‡Œå‡è®¾ä¸º RGB å›¾åƒï¼ˆ3 ä¸ªé€šé“ï¼‰ã€‚
        # nc=nc æ˜¯ç±»åˆ«æ•°ï¼Œå‰é¢å·²ç»è®¾ç½®ã€‚
        # anchors=hyp.get("anchors") æ˜¯ä½¿ç”¨è¶…å‚æ•°ä¸­å®šä¹‰çš„ anchorã€‚
        model = Model(cfg or ckpt["model"].yaml, ch=3, nc=nc, anchors=hyp.get("anchors")).to(device)  # create
        # å¦‚æœæ¨¡å‹çš„é…ç½®æˆ–è¶…å‚æ•°ä¸­åŒ…å« anchorsï¼Œå¹¶ä¸”ä¸éœ€è¦æ¢å¤ï¼ˆå³ä¸ä½¿ç”¨ resumeï¼‰ï¼Œåˆ™æ’é™¤ anchor éƒ¨åˆ†çš„æƒé‡ã€‚
        exclude = ["anchor"] if (cfg or hyp.get("anchors")) and not resume else []  # exclude keys
        # è·å–é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡ï¼Œè½¬æ¢ä¸º FP32 ç²¾åº¦ã€‚
        csd = ckpt["model"].float().state_dict()  # checkpoint state_dict as FP32
        # å°†é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡ä¸å½“å‰æ¨¡å‹çš„æƒé‡è¿›è¡Œäº¤é›†æ“ä½œï¼ŒåªåŠ è½½åŒ¹é…çš„æƒé‡ã€‚
        csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # intersect
        # åŠ è½½æƒé‡åˆ°æ¨¡å‹ä¸­ï¼Œstrict=False è¡¨ç¤ºä¸ä¸¥æ ¼è¦æ±‚æ‰€æœ‰å±‚éƒ½åŒ¹é…ï¼Œå…è®¸æŸäº›å±‚ä¸åŒ¹é…ï¼ˆä¾‹å¦‚ï¼Œæ’é™¤çš„ anchors å±‚ï¼‰ã€‚
        model.load_state_dict(csd, strict=False)  # load
        # è®°å½•å·²ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­æˆåŠŸåŠ è½½äº†å¤šå°‘ä¸ªå‚æ•°ã€‚
        LOGGER.info(f"Transferred {len(csd)}/{len(model.state_dict())} items from {weights}")  # report
    else:
        # å¦‚æœæ²¡æœ‰é¢„è®­ç»ƒæƒé‡ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°æ¨¡å‹
        model = Model(cfg, ch=3, nc=nc, anchors=hyp.get("anchors")).to(device)  # create
    # æ£€æŸ¥è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰
    # AMP æœ‰åŠ©äºæé«˜è®­ç»ƒæ•ˆç‡å¹¶èŠ‚çœå†…å­˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨ GPU æ—¶
    amp = check_amp(model)  # check AMP

    # Freeze
    # freeze å˜é‡æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œè¡¨ç¤ºéœ€è¦å†»ç»“çš„å±‚çš„åç§°ã€‚
    # å¦‚æœ freeze åˆ—è¡¨çš„é•¿åº¦å¤§äº 1ï¼Œåˆ™ä½¿ç”¨ç»™å®šçš„ freeze å€¼ä½œä¸ºå±‚ååˆ—è¡¨ã€‚å¦‚æœ freeze åªæœ‰ä¸€ä¸ªå…ƒç´ ï¼Œåˆ™è®¤ä¸ºå®ƒæ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤ºè¦å†»ç»“çš„å±‚çš„èŒƒå›´ã€‚
    # f"model.{x}." æ˜¯ç”¨æ¥æ ¼å¼åŒ–æ¯ä¸ªå±‚åç§°çš„å­—ç¬¦ä¸²ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ freeze = [0, 1]ï¼Œåˆ™ä¼šç”Ÿæˆ ["model.0.", "model.1."]ï¼Œç”¨äºè¡¨ç¤º model æ¨¡å‹ä¸­çš„å‰ä¸¤å±‚ã€‚
    freeze = [f"model.{x}." for x in (freeze if len(freeze) > 1 else range(freeze[0]))]  # layers to freeze
    # model.named_parameters() ä¼šè¿”å›æ¨¡å‹ä¸­æ‰€æœ‰çš„å‚æ•°åŠå…¶å¯¹åº”çš„åç§°
    for k, v in model.named_parameters():
        # é»˜è®¤æƒ…å†µä¸‹ï¼Œv.requires_grad = Trueï¼Œè¿™è¡¨ç¤ºæ‰€æœ‰å‚æ•°çš„æ¢¯åº¦æ˜¯è®¡ç®—çš„ï¼Œæ„å‘³ç€æ‰€æœ‰å±‚é»˜è®¤éƒ½å¯ä»¥è®­ç»ƒã€‚
        v.requires_grad = True  # train all layers
        # v.register_hook(lambda x: torch.nan_to_num(x))  # NaN to 0 (commented for erratic training results)
        # å¦‚æœå½“å‰å±‚çš„åç§° k åŒ…å«åœ¨ freeze åˆ—è¡¨ä¸­ï¼Œé‚£ä¹ˆè¿™å±‚å°±ä¼šè¢«å†»ç»“ã€‚
        # any(x in k for x in freeze) æ˜¯ä¸€ä¸ªæ¡ä»¶åˆ¤æ–­ï¼Œè¡¨ç¤ºå¦‚æœ kï¼ˆå±‚åç§°ï¼‰ä¸­åŒ…å« freeze ä¸­çš„ä»»æ„ä¸€ä¸ªå…ƒç´ ï¼Œåˆ™å°†è¯¥å±‚çš„ requires_grad è®¾ç½®ä¸º Falseï¼Œå³å†»ç»“è¯¥å±‚çš„å‚æ•°ã€‚
        # LOGGER.info(f"freezing {k}") ä¼šè¾“å‡ºä¸€æ¡æ—¥å¿—ï¼Œè®°å½•è¢«å†»ç»“çš„å±‚çš„åç§°ã€‚
        # v.requires_grad = False å°†è¯¥å±‚çš„ requires_grad è®¾ç½®ä¸º Falseï¼Œè¡¨ç¤ºè¿™ä¸ªå±‚çš„å‚æ•°ä¸å‚ä¸è®­ç»ƒã€‚
        if any(x in k for x in freeze):
            LOGGER.info(f"freezing {k}")
            v.requires_grad = False

    # Image size
    # model.stride.max()ï¼šstride è¡¨ç¤ºæ¨¡å‹ä¸­å„å±‚çš„æ­¥å¹…ï¼ˆstrideï¼‰ï¼Œå®ƒæ˜¯ç½‘ç»œä¸­æ¯ä¸ªå·ç§¯å±‚æˆ–æ± åŒ–å±‚çš„æ­¥é•¿ã€‚
    # model.stride æ˜¯ä¸€ä¸ªåŒ…å«æ¯å±‚æ­¥å¹…çš„å¼ é‡ã€‚stride.max() æ˜¯è·å–æ¨¡å‹ä¸­æœ€å¤§æ­¥å¹…å€¼ã€‚
    # max(int(model.stride.max()), 32)ï¼šé€‰æ‹© stride.max() å’Œ 32 ä¹‹é—´çš„è¾ƒå¤§å€¼ä½œä¸º gsã€‚
    # æ­¥å¹…çš„å¤§å°å†³å®šäº†å›¾åƒçš„ç½‘æ ¼åˆ’åˆ†ï¼Œå› æ­¤éœ€è¦ç¡®ä¿å›¾åƒå°ºå¯¸æ˜¯æ­¥å¹…çš„å€æ•°ï¼Œé¿å…åœ¨ç½‘ç»œä¸­å‡ºç°ä¸è§„åˆ™çš„å°ºå¯¸ã€‚
    gs = max(int(model.stride.max()), 32)  # grid size (max stride)
    # check_img_size(opt.imgsz, gs, floor=gs * 2)ï¼šè¯¥å‡½æ•°ç”¨äºæ£€æŸ¥å¹¶ç¡®ä¿ä¼ å…¥çš„å›¾åƒå°ºå¯¸ï¼ˆopt.imgszï¼‰æ˜¯ gs çš„å€æ•°ã€‚
    # gs æ˜¯æœ€å¤§æ­¥å¹…ï¼Œå›¾åƒå°ºå¯¸éœ€è¦æ˜¯ gs çš„æ•´æ•°å€ï¼Œä»¥ç¡®ä¿åœ¨ç½‘ç»œä¸­å¤„ç†æ—¶æ²¡æœ‰å°ºå¯¸ä¸åŒ¹é…çš„é—®é¢˜ã€‚
    # floor=gs * 2ï¼šè¿™è¡¨ç¤ºå¦‚æœè¾“å…¥å›¾åƒå°ºå¯¸ä¸ç¬¦åˆè¦æ±‚ï¼Œå¯ä»¥å°†å…¶è°ƒæ•´ä¸º gs çš„å€æ•°ï¼Œä¸”ä¸å°äº gs * 2ã€‚
    # floor å‚æ•°ç¡®ä¿åœ¨è°ƒæ•´å›¾åƒå°ºå¯¸æ—¶ä¸ä¼šå¤ªå°ã€‚
    imgsz = check_img_size(opt.imgsz, gs, floor=gs * 2)  # verify imgsz is gs-multiple

    # Batch size
    # RANK == -1ï¼šè¿™æ„å‘³ç€å½“å‰æ¨¡å‹ä¸æ˜¯åœ¨åˆ†å¸ƒå¼è®­ç»ƒæ¨¡å¼ä¸‹è¿è¡Œï¼Œè€Œæ˜¯å•GPUæ¨¡å¼ã€‚
    # RANK ç”¨äºæ ‡è¯†ä¸åŒçš„è®­ç»ƒè¿›ç¨‹ï¼Œåœ¨å•GPUæ¨¡å¼ä¸‹ï¼ŒRANK çš„å€¼ä¸º -1ã€‚
    # batch_size == -1ï¼šè¡¨ç¤ºå½“å‰æ‰¹é‡å¤§å°æœªæŒ‡å®šæˆ–ä¸ºé»˜è®¤å€¼ï¼ˆé€šå¸¸æ˜¯ -1ï¼‰ï¼Œæ­¤æ—¶éœ€è¦ä¼°ç®—æœ€ä½³çš„æ‰¹é‡å¤§å°ã€‚
    if RANK == -1 and batch_size == -1:  # single-GPU only, estimate best batch size
        # check_train_batch_size(model, imgsz, amp)ï¼šè¿™æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œ
        # ç”¨æ¥æ ¹æ®å½“å‰æ¨¡å‹ï¼ˆmodelï¼‰ã€è¾“å…¥å›¾åƒå°ºå¯¸ï¼ˆimgszï¼‰å’Œæ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆampï¼‰æ¥ä¼°ç®—æœ€åˆé€‚çš„æ‰¹é‡å¤§å°ã€‚
        # check_train_batch_size ä¼šæ ¹æ®å¯ç”¨çš„ GPU å†…å­˜ä»¥åŠæ¨¡å‹å’Œå›¾åƒçš„å¤§å°ï¼Œè®¡ç®—å‡ºä¸€ä¸ªåˆé€‚çš„æ‰¹é‡å¤§å°ã€‚
        # è¾ƒå¤§çš„æ‰¹é‡å¤§å°é€šå¸¸ä¼šåŠ é€Ÿè®­ç»ƒï¼Œä½†éœ€è¦æ›´å¤šçš„ GPU å†…å­˜ã€‚
        batch_size = check_train_batch_size(model, imgsz, amp)
        loggers.on_params_update({"batch_size": batch_size})

    # Optimizer
    # nbs = 64ï¼šnbs ä»£è¡¨â€œåä¹‰æ‰¹é‡å¤§å°â€ï¼Œè¿™æ˜¯ä¼˜åŒ–å™¨è°ƒæ•´çš„ä¸€ä¸ªåŸºå‡†å€¼ã€‚
    # å®ƒæŒ‡ç¤ºäº†ç†æƒ³çš„æ‰¹é‡å¤§å°ï¼Œä»¥ä¾¿ä¼˜åŒ–å™¨åœ¨è¿™ä¸ªæ‰¹é‡å¤§å°ä¸‹è¡¨ç°æœ€ä½³ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåä¹‰æ‰¹é‡å¤§å°è¢«è®¾ä¸º 64ã€‚
    nbs = 64  # nominal batch size
    # accumulate æ˜¯æ¢¯åº¦ç´¯ç§¯çš„æ¬¡æ•°ã€‚æ¢¯åº¦ç´¯ç§¯æ˜¯ä¸€ä¸ªåœ¨æ˜¾å­˜å—é™æ—¶æœ‰æ•ˆçš„æŠ€æœ¯ï¼Œå®ƒå…è®¸æ¨¡å‹åœ¨å¤šä¸ªå°æ‰¹é‡ä¸Šè®¡ç®—æ¢¯åº¦ï¼Œç„¶åä¸€æ¬¡æ€§æ›´æ–°æƒé‡ã€‚
    # è¿™æ®µä»£ç é€šè¿‡å°†åä¹‰æ‰¹é‡å¤§å°é™¤ä»¥å½“å‰æ‰¹é‡å¤§å°ï¼Œæ¥è®¡ç®—æ¢¯åº¦ç´¯ç§¯çš„æ¬¡æ•°ã€‚
    # ä¾‹å¦‚ï¼Œå¦‚æœå®é™…æ‰¹é‡å¤§å°ä¸º 32ï¼Œé‚£ä¹ˆéœ€è¦ç´¯ç§¯ 64 / 32 = 2 æ¬¡æ¢¯åº¦ã€‚
    # å¦‚æœæ‰¹é‡å¤§å°æ¯”åä¹‰æ‰¹é‡å¤§å°å¤§ï¼Œåˆ™ç´¯ç§¯æ¬¡æ•°ä¸º 1ã€‚
    accumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizing
    # è°ƒæ•´æƒé‡è¡°å‡ (weight_decay)
    # è¿™é‡Œè°ƒæ•´äº† weight_decay çš„å€¼ã€‚weight_decay æ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œç”¨äºé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆã€‚
    # æƒé‡è¡°å‡æ˜¯æ ¹æ®å®é™…çš„æ‰¹é‡å¤§å°å’Œç´¯ç§¯æ¬¡æ•°è¿›è¡Œäº†ç¼©æ”¾ã€‚
    # å› ä¸º weight_decay çš„æ•ˆæœé€šå¸¸ä¸æ‰¹é‡å¤§å°æœ‰å…³ï¼Œæ‰€ä»¥æˆ‘ä»¬æŒ‰æ¯”ä¾‹è°ƒæ•´å®ƒä»¥é€‚åº”å½“å‰çš„æ‰¹é‡å¤§å°ã€‚
    hyp["weight_decay"] *= batch_size * accumulate / nbs  # scale weight_decay
    # smart_optimizer æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œç”¨æ¥åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªä¼˜åŒ–å™¨å®ä¾‹ã€‚è¯¥ä¼˜åŒ–å™¨å°†ç”¨äºè®­ç»ƒè¿‡ç¨‹ä¸­çš„å‚æ•°æ›´æ–°ã€‚
    optimizer = smart_optimizer(model, opt.optimizer, hyp["lr0"], hyp["momentum"], hyp["weight_decay"])

    # Scheduler å­¦ä¹ ç‡è°ƒåº¦å™¨
    # ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦ (opt.cos_lr)
    if opt.cos_lr:
        # one_cycle æ˜¯ä¸€ç§å¸¸è§çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼Œå®ƒä¼šå°†å­¦ä¹ ç‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å…ˆå‡é«˜å†é™ä¸‹æ¥ï¼Œé€šå¸¸ç”¨äºå¿«é€Ÿæ”¶æ•›ã€‚
        # è¿™é‡Œçš„ hyp["lrf"] æ˜¯æœ€ç»ˆå­¦ä¹ ç‡ï¼ˆé€šå¸¸æ˜¯è¾ƒå°çš„å€¼ï¼‰ï¼Œepochs æ˜¯æ€»è®­ç»ƒè½®æ•°ã€‚
        lf = one_cycle(1, hyp["lrf"], epochs)  # cosine 1->hyp['lrf']
    else:
        # æ²¡æœ‰å¯ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦ï¼ˆå³ opt.cos_lr ä¸º Falseï¼‰ï¼Œåˆ™ä½¿ç”¨çº¿æ€§è¡°å‡çš„å­¦ä¹ ç‡è°ƒåº¦ã€‚
        # lf(x) æ˜¯ä¸€ä¸ªçº¿æ€§è¡°å‡å­¦ä¹ ç‡çš„è®¡ç®—å‡½æ•°ï¼Œå…¶ä¸­ x è¡¨ç¤ºå½“å‰çš„è®­ç»ƒè½®æ¬¡ï¼ˆepochï¼‰
        def lf(x):
            """Linear learning rate scheduler function with decay calculated by epoch proportion."""
            # åœ¨è®­ç»ƒå¼€å§‹æ—¶ï¼ˆx = 0ï¼‰ï¼Œå­¦ä¹ ç‡æ˜¯æœ€å¤§å€¼ 1.0 - hyp["lrf"]ï¼›éšç€è®­ç»ƒè¿›è¡Œï¼ˆx å¢åŠ ï¼‰ï¼Œå­¦ä¹ ç‡é€æ¸ä¸‹é™ï¼Œç›´åˆ°æœ€ç»ˆæ¥è¿‘ hyp["lrf"]ã€‚
            return (1 - x / epochs) * (1.0 - hyp["lrf"]) + hyp["lrf"]  # linear
    # åˆ›å»ºä¸€ä¸ªå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆLambdaLRï¼‰ï¼Œå®ƒåŸºäºè‡ªå®šä¹‰çš„å­¦ä¹ ç‡è°ƒæ•´å‡½æ•°ï¼ˆlfï¼‰æ¥è°ƒæ•´å­¦ä¹ ç‡ã€‚
    # lr_scheduler.LambdaLR:
    #   LambdaLR æ˜¯ PyTorch ä¸­çš„ä¸€ç§å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œå®ƒä½¿ç”¨ä¸€ä¸ªè‡ªå®šä¹‰çš„å‡½æ•° lr_lambda æ¥è°ƒæ•´å­¦ä¹ ç‡ã€‚
    #   LambdaLR é€šè¿‡åœ¨æ¯ä¸ªè®­ç»ƒè½®æ¬¡ï¼ˆepochï¼‰æ—¶è°ƒç”¨ lr_lambda å‡½æ•°æ¥è®¡ç®—æ–°çš„å­¦ä¹ ç‡ã€‚
    #   å®ƒéå¸¸çµæ´»ï¼Œå¯ä»¥è®©ä½ å®šä¹‰ä»»æ„å½¢å¼çš„å­¦ä¹ ç‡è¡°å‡ç­–ç•¥ï¼Œåªéœ€æä¾›ä¸€ä¸ªæ ¹æ®å½“å‰è½®æ¬¡ xï¼ˆæˆ–è€…å…¶ä»–æ ‡å‡†ï¼‰è®¡ç®—å­¦ä¹ ç‡çš„å‡½æ•°ã€‚
    # optimizer:
    #   optimizer æ˜¯ä½ å®šä¹‰çš„ä¼˜åŒ–å™¨ï¼ˆå¦‚ torch.optim.SGD æˆ– torch.optim.Adamï¼‰ã€‚
    #   è¿™ä¸ªä¼˜åŒ–å™¨åŒ…å«äº†æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ä»¥åŠç›¸å…³çš„è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€åŠ¨é‡ç­‰ï¼‰ã€‚LambdaLR ä¼šæ ¹æ®è¿™ä¸ªä¼˜åŒ–å™¨ä¸­çš„å­¦ä¹ ç‡æ¥è°ƒæ•´å‚æ•°ã€‚
    # lr_lambda=lf:
    #   lf æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒå®šä¹‰äº†å­¦ä¹ ç‡å¦‚ä½•éšç€è®­ç»ƒè½®æ¬¡å˜åŒ–ã€‚lf çš„è¾“å…¥é€šå¸¸æ˜¯å½“å‰è®­ç»ƒè½®æ¬¡ xï¼Œå¹¶ä¸”è¿”å›ä¸€ä¸ªè°ƒæ•´åçš„å­¦ä¹ ç‡ã€‚
    #   è¿™ä¸ªå‡½æ•°ä¼šæ ¹æ®è®­ç»ƒè½®æ¬¡è°ƒæ•´å­¦ä¹ ç‡ï¼Œå¯ä»¥æ˜¯ä½™å¼¦è¡°å‡ã€çº¿æ€§è¡°å‡ç­‰ï¼Œå…·ä½“å–å†³äºä½ åœ¨å‰é¢ä»£ç ä¸­å®šä¹‰çš„è°ƒåº¦é€»è¾‘ã€‚
    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)  # plot_lr_scheduler(optimizer, scheduler, epochs)

    # EMA
    # ModelEMA æ˜¯ä¸€ä¸ªç±»ï¼Œé€šå¸¸ç”¨äºå®ç° æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰ã€‚
    # å®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒæ¨¡å‹æƒé‡çš„ä¸€ä¸ªå¹³æ»‘ç‰ˆæœ¬ï¼Œä»¥å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­æƒé‡æ³¢åŠ¨çš„å½±å“ã€‚
    ema = ModelEMA(model) if RANK in {-1, 0} else None

    # Resume
    # best_fitness: è®°å½•å½“å‰æœ€å¥½çš„è®­ç»ƒæ€§èƒ½ï¼Œé€šå¸¸æ˜¯æŒ‡åœ¨éªŒè¯é›†ä¸Šæ¨¡å‹çš„æœ€ä¼˜å‡†ç¡®åº¦ï¼ˆä¾‹å¦‚ mAPï¼‰ã€‚
    # start_epoch: ä»å“ªé‡Œå¼€å§‹æ¢å¤è®­ç»ƒï¼Œå¦‚æœæ¨¡å‹æ¢å¤è‡ªæŸä¸ªæ£€æŸ¥ç‚¹ï¼ˆcheckpointï¼‰ï¼Œé‚£ä¹ˆä¼šç”¨è¿™ä¸ªå€¼æ¥æŒ‡å®šä»å“ªä¸ª epoch å¼€å§‹è®­ç»ƒã€‚
    best_fitness, start_epoch = 0.0, 0
    if pretrained:
        # resume ä¸º True è¡¨ç¤ºéœ€è¦ä»ä¹‹å‰çš„è®­ç»ƒçŠ¶æ€æ¢å¤
        if resume:
            best_fitness, start_epoch, epochs = smart_resume(ckpt, optimizer, ema, weights, epochs, resume)
        # åˆ é™¤ä¸éœ€è¦çš„å˜é‡,è¿™äº›å˜é‡é€šå¸¸åªåœ¨æ¢å¤æ—¶ä½¿ç”¨ï¼Œä¸å†éœ€è¦ï¼Œå› æ­¤é‡Šæ”¾å†…å­˜.
        del ckpt, csd

    # DP mode
    # è®¾ç½® DataParallel (DP) æ¨¡å¼æ¥æ”¯æŒå¤š GPU è®­ç»ƒ
    # cuda: åˆ¤æ–­å½“å‰è®¾å¤‡æ˜¯å¦ä¸º GPUã€‚device.type != "cpu" å¦‚æœæ˜¯ GPUï¼Œcuda ä¸º Trueã€‚
    # RANK == -1: è¿™æ˜¯åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­åˆ¤æ–­å½“å‰çš„è¿›ç¨‹æ˜¯å¦æ˜¯ä¸»è¿›ç¨‹ï¼ˆrank -1 è¡¨ç¤ºå•æœºè®­ç»ƒæˆ–éåˆ†å¸ƒå¼ç¯å¢ƒï¼‰ã€‚
    #   åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼ŒRANK ä¼šè¢«è®¾ç½®ä¸ºå¤§äºç­‰äº 0 çš„å€¼ï¼Œè¡¨ç¤ºå½“å‰è¿›ç¨‹çš„æ’åã€‚
    # torch.cuda.device_count() > 1: æ£€æŸ¥å½“å‰ç³»ç»Ÿæ˜¯å¦æœ‰å¤šä¸ª GPUã€‚å¦‚æœ GPU æ•°é‡å¤§äº 1ï¼Œè¯´æ˜æœ‰å¤šå¡è®­ç»ƒçš„æ¡ä»¶ã€‚
    if cuda and RANK == -1 and torch.cuda.device_count() > 1:
        # å¦‚æœæ¡ä»¶æˆç«‹ï¼Œå³å­˜åœ¨å¤šä¸ª GPUï¼Œä¸”æ²¡æœ‰ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œç¨‹åºä¼šè¾“å‡ºä¸€æ¡è­¦å‘Šä¿¡æ¯ï¼Œ
        # æç¤ºç”¨æˆ· DataParallel (DP) æ¨¡å¼ä¸æ˜¯æ¨èçš„å¤š GPU è®­ç»ƒæ–¹å¼ã€‚
        # æ¨èç”¨æˆ·ä½¿ç”¨ Distributed Data Parallel (DDP)ï¼Œå› ä¸º DDP åœ¨å¤š GPU è®­ç»ƒä¸­çš„æ€§èƒ½å’Œæ•ˆç‡æ¯” DP æ›´å¥½ã€‚
        LOGGER.warning(
            "WARNING âš ï¸ DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\n"
            "See Multi-GPU Tutorial at https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training to get started."
        )
        # torch.nn.DataParallel æ˜¯ PyTorch ä¸­çš„ä¸€ç§å¤š GPU å¹¶è¡Œè®­ç»ƒçš„æ–¹å¼ã€‚
        # å®ƒä¼šå°†æ¨¡å‹çš„è¾“å…¥æ•°æ®åˆ’åˆ†æˆå¤šä¸ªå­æ‰¹æ¬¡ï¼ˆsub-batchï¼‰ï¼Œç„¶ååœ¨æ¯ä¸ª GPU ä¸Šè¿›è¡Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼Œ
        # æœ€åå°†æ¢¯åº¦æ±‡æ€»å¹¶æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
        model = torch.nn.DataParallel(model)

    # SyncBatchNorm åŒæ­¥æ‰¹å½’ä¸€åŒ–-åœ¨å¤š GPU åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œç¡®ä¿æ‰€æœ‰ GPU ä¸Šçš„æ‰¹å½’ä¸€åŒ–å±‚ä½¿ç”¨åŒæ­¥æ–¹å¼ï¼Œä»è€Œæå‡è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ”¶æ•›æ•ˆæœ
    if opt.sync_bn and cuda and RANK != -1:
        # å°†æ¨¡å‹ä¸­çš„æ‰€æœ‰ BatchNorm å±‚è½¬æ¢ä¸ºåŒæ­¥æ‰¹å½’ä¸€åŒ–å±‚ï¼ˆSyncBatchNormï¼‰ã€‚
        # è¿™æ„å‘³ç€åœ¨å¤š GPU ç¯å¢ƒä¸‹ï¼Œæ‰€æœ‰ GPU ä¸Šçš„ BatchNorm å±‚å°†å…±äº«åŒä¸€ä¸ªå‡å€¼å’Œæ–¹å·®ç»Ÿè®¡å€¼ï¼Œè€Œä¸æ˜¯æ¯ä¸ª GPU å•ç‹¬è®¡ç®—ã€‚
        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)
        LOGGER.info("Using SyncBatchNorm()")

    # Trainloader
    train_loader, dataset = create_dataloader(
        train_path, # è®­ç»ƒæ•°æ®çš„è·¯å¾„
        imgsz, # è®­ç»ƒæ•°æ®çš„è·¯å¾„
        batch_size // WORLD_SIZE, # æ¯ä¸ª GPU çš„æ‰¹é‡å¤§å°
        gs, # grid size
        single_cls, # æ˜¯å¦ä¸ºå•ç±»è®­ç»ƒï¼ˆç”¨äºå¤„ç†åªæœ‰ä¸€ä¸ªç±»åˆ«çš„æƒ…å†µï¼‰
        hyp=hyp, # è¶…å‚æ•°é…ç½®
        augment=True, # æ˜¯å¦è¿›è¡Œæ•°æ®å¢å¼º
        cache=None if opt.cache == "val" else opt.cache, # æ˜¯å¦ç¼“å­˜æ•°æ®,å¯ä»¥æŠŠæ‰€æœ‰çš„æ•°æ®éƒ½ç¼“å­˜ä¸‹æ¥
        rect=opt.rect, # æ˜¯å¦è¿›è¡ŒçŸ©å½¢è®­ç»ƒï¼ˆä¿æŒåŸå§‹é•¿å®½æ¯”ï¼‰
        rank=LOCAL_RANK, # å½“å‰è¿›ç¨‹çš„ rankï¼ˆåœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ä½¿ç”¨ï¼‰
        workers=workers, # ç”¨äºåŠ è½½æ•°æ®çš„çº¿ç¨‹æ•°
        image_weights=opt.image_weights,  # æ˜¯å¦ä½¿ç”¨å›¾åƒæƒé‡
        quad=opt.quad, # æ˜¯å¦å¯ç”¨å››å…ƒç»„æ•°æ®å¢å¼º
        prefix=colorstr("train: "), # ç”¨äºæ—¥å¿—è¾“å‡ºçš„å‰ç¼€
        shuffle=True, # æ˜¯å¦æ‰“ä¹±æ•°æ®
        seed=opt.seed, # éšæœºç§å­
    )
    # å°†æ‰€æœ‰æ•°æ®é›†ä¸­çš„æ ‡ç­¾åˆå¹¶æˆä¸€ä¸ªå¤§çš„ NumPy æ•°ç»„
    labels = np.concatenate(dataset.labels, 0)
    # é€šè¿‡è·å–æ ‡ç­¾æ•°ç»„ä¸­æœ€å¤§å€¼æ¥è·å¾—æ•°æ®é›†ä¸­æœ€å¤§çš„æ ‡ç­¾ç±»åˆ«ã€‚
    # labels[:, 0] é€‰å–æ ‡ç­¾æ•°ç»„ä¸­çš„ç¬¬ä¸€åˆ—ï¼ˆå³ç±»åˆ«æ ‡ç­¾ï¼‰ï¼Œç„¶åè°ƒç”¨ .max() å¾—åˆ°æœ€å¤§çš„ç±»æ ‡ç­¾ã€‚
    # mlc ä»£è¡¨æœ€å¤§ç±»æ ‡ç­¾ï¼Œç”¨äºåç»­éªŒè¯ç±»æ ‡ç­¾çš„åˆæ³•æ€§ã€‚
    mlc = int(labels[:, 0].max())  # max label class
    # è¿™ä¸ªæ–­è¨€ç¡®ä¿æ•°æ®é›†ä¸­çš„æœ€å¤§æ ‡ç­¾ç±»ä¸ä¼šè¶…è¿‡æ¨¡å‹å®šä¹‰çš„ç±»åˆ«æ•°
    assert mlc < nc, f"Label class {mlc} exceeds nc={nc} in {data}. Possible class labels are 0-{nc - 1}"

    # Process 0
    if RANK in {-1, 0}:
        val_loader = create_dataloader(
            val_path,
            imgsz,
            batch_size // WORLD_SIZE * 2,
            gs,
            single_cls,
            hyp=hyp,
            cache=None if noval else opt.cache,
            rect=True,
            rank=-1,
            workers=workers * 2,
            pad=0.5,
            prefix=colorstr("val: "),
        )[0]

        if not resume:
            if not opt.noautoanchor:
                check_anchors(dataset, model=model, thr=hyp["anchor_t"], imgsz=imgsz)  # run AutoAnchor
            model.half().float()  # pre-reduce anchor precision

        callbacks.run("on_pretrain_routine_end", labels, names)

    # DDP mode
    if cuda and RANK != -1:
        model = smart_DDP(model)

    # Model attributes
    nl = de_parallel(model).model[-1].nl  # number of detection layers (to scale hyps)
    hyp["box"] *= 3 / nl  # scale to layers
    hyp["cls"] *= nc / 80 * 3 / nl  # scale to classes and layers
    hyp["obj"] *= (imgsz / 640) ** 2 * 3 / nl  # scale to image size and layers
    hyp["label_smoothing"] = opt.label_smoothing
    model.nc = nc  # attach number of classes to model
    model.hyp = hyp  # attach hyperparameters to model
    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  # attach class weights
    model.names = names

    # Start training
    t0 = time.time()
    nb = len(train_loader)  # number of batches
    nw = max(round(hyp["warmup_epochs"] * nb), 100)  # number of warmup iterations, max(3 epochs, 100 iterations)
    # nw = min(nw, (epochs - start_epoch) / 2 * nb)  # limit warmup to < 1/2 of training
    last_opt_step = -1
    maps = np.zeros(nc)  # mAP per class
    results = (0, 0, 0, 0, 0, 0, 0)  # P, R, mAP@.5, mAP@.5-.95, val_loss(box, obj, cls)
    scheduler.last_epoch = start_epoch - 1  # do not move
    scaler = torch.cuda.amp.GradScaler(enabled=amp)
    stopper, stop = EarlyStopping(patience=opt.patience), False
    compute_loss = ComputeLoss(model)  # init loss class
    callbacks.run("on_train_start")
    LOGGER.info(
        f'Image sizes {imgsz} train, {imgsz} val\n'
        f'Using {train_loader.num_workers * WORLD_SIZE} dataloader workers\n'
        f"Logging results to {colorstr('bold', save_dir)}\n"
        f'Starting training for {epochs} epochs...'
    )
    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------
        callbacks.run("on_train_epoch_start")
        model.train()

        # Update image weights (optional, single-GPU only)
        if opt.image_weights:
            cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  # class weights
            iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  # image weights
            dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  # rand weighted idx

        # Update mosaic border (optional)
        # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)
        # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders

        mloss = torch.zeros(3, device=device)  # mean losses
        if RANK != -1:
            train_loader.sampler.set_epoch(epoch)
        pbar = enumerate(train_loader)
        LOGGER.info(("\n" + "%11s" * 7) % ("Epoch", "GPU_mem", "box_loss", "obj_loss", "cls_loss", "Instances", "Size"))
        if RANK in {-1, 0}:
            pbar = tqdm(pbar, total=nb, bar_format=TQDM_BAR_FORMAT)  # progress bar
        optimizer.zero_grad()
        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------
            callbacks.run("on_train_batch_start")
            ni = i + nb * epoch  # number integrated batches (since train start)
            imgs = imgs.to(device, non_blocking=True).float() / 255  # uint8 to float32, 0-255 to 0.0-1.0

            # Warmup
            if ni <= nw:
                xi = [0, nw]  # x interp
                # compute_loss.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)
                accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())
                for j, x in enumerate(optimizer.param_groups):
                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0
                    x["lr"] = np.interp(ni, xi, [hyp["warmup_bias_lr"] if j == 0 else 0.0, x["initial_lr"] * lf(epoch)])
                    if "momentum" in x:
                        x["momentum"] = np.interp(ni, xi, [hyp["warmup_momentum"], hyp["momentum"]])

            # Multi-scale
            if opt.multi_scale:
                sz = random.randrange(int(imgsz * 0.5), int(imgsz * 1.5) + gs) // gs * gs  # size
                sf = sz / max(imgs.shape[2:])  # scale factor
                if sf != 1:
                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)
                    imgs = nn.functional.interpolate(imgs, size=ns, mode="bilinear", align_corners=False)

            # Forward
            with torch.cuda.amp.autocast(amp):
                pred = model(imgs)  # forward
                loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size
                if RANK != -1:
                    loss *= WORLD_SIZE  # gradient averaged between devices in DDP mode
                if opt.quad:
                    loss *= 4.0

            # Backward
            scaler.scale(loss).backward()

            # Optimize - https://pytorch.org/docs/master/notes/amp_examples.html
            if ni - last_opt_step >= accumulate:
                scaler.unscale_(optimizer)  # unscale gradients
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
                scaler.step(optimizer)  # optimizer.step
                scaler.update()
                optimizer.zero_grad()
                if ema:
                    ema.update(model)
                last_opt_step = ni

            # Log
            if RANK in {-1, 0}:
                mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses
                mem = f"{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G"  # (GB)
                pbar.set_description(
                    ("%11s" * 2 + "%11.4g" * 5)
                    % (f"{epoch}/{epochs - 1}", mem, *mloss, targets.shape[0], imgs.shape[-1])
                )
                callbacks.run("on_train_batch_end", model, ni, imgs, targets, paths, list(mloss))
                if callbacks.stop_training:
                    return
            # end batch ------------------------------------------------------------------------------------------------

        # Scheduler
        lr = [x["lr"] for x in optimizer.param_groups]  # for loggers
        scheduler.step()

        if RANK in {-1, 0}:
            # mAP
            callbacks.run("on_train_epoch_end", epoch=epoch)
            ema.update_attr(model, include=["yaml", "nc", "hyp", "names", "stride", "class_weights"])
            final_epoch = (epoch + 1 == epochs) or stopper.possible_stop
            if not noval or final_epoch:  # Calculate mAP
                results, maps, _ = validate.run(
                    data_dict,
                    batch_size=batch_size // WORLD_SIZE * 2,
                    imgsz=imgsz,
                    half=amp,
                    model=ema.ema,
                    single_cls=single_cls,
                    dataloader=val_loader,
                    save_dir=save_dir,
                    plots=False,
                    callbacks=callbacks,
                    compute_loss=compute_loss,
                )

            # Update best mAP
            fi = fitness(np.array(results).reshape(1, -1))  # weighted combination of [P, R, mAP@.5, mAP@.5-.95]
            stop = stopper(epoch=epoch, fitness=fi)  # early stop check
            if fi > best_fitness:
                best_fitness = fi
            log_vals = list(mloss) + list(results) + lr
            callbacks.run("on_fit_epoch_end", log_vals, epoch, best_fitness, fi)

            # Save model
            if (not nosave) or (final_epoch and not evolve):  # if save
                ckpt = {
                    "epoch": epoch,
                    "best_fitness": best_fitness,
                    "model": deepcopy(de_parallel(model)).half(),
                    "ema": deepcopy(ema.ema).half(),
                    "updates": ema.updates,
                    "optimizer": optimizer.state_dict(),
                    "opt": vars(opt),
                    "git": GIT_INFO,  # {remote, branch, commit} if a git repo
                    "date": datetime.now().isoformat(),
                }

                # Save last, best and delete
                torch.save(ckpt, last)
                if best_fitness == fi:
                    torch.save(ckpt, best)
                if opt.save_period > 0 and epoch % opt.save_period == 0:
                    torch.save(ckpt, w / f"epoch{epoch}.pt")
                del ckpt
                callbacks.run("on_model_save", last, epoch, final_epoch, best_fitness, fi)

        # EarlyStopping
        if RANK != -1:  # if DDP training
            broadcast_list = [stop if RANK == 0 else None]
            dist.broadcast_object_list(broadcast_list, 0)  # broadcast 'stop' to all ranks
            if RANK != 0:
                stop = broadcast_list[0]
        if stop:
            break  # must break all DDP ranks

        # end epoch ----------------------------------------------------------------------------------------------------
    # end training -----------------------------------------------------------------------------------------------------
    if RANK in {-1, 0}:
        LOGGER.info(f"\n{epoch - start_epoch + 1} epochs completed in {(time.time() - t0) / 3600:.3f} hours.")
        for f in last, best:
            if f.exists():
                strip_optimizer(f)  # strip optimizers
                if f is best:
                    LOGGER.info(f"\nValidating {f}...")
                    results, _, _ = validate.run(
                        data_dict,
                        batch_size=batch_size // WORLD_SIZE * 2,
                        imgsz=imgsz,
                        model=attempt_load(f, device).half(),
                        iou_thres=0.65 if is_coco else 0.60,  # best pycocotools at iou 0.65
                        single_cls=single_cls,
                        dataloader=val_loader,
                        save_dir=save_dir,
                        save_json=is_coco,
                        verbose=True,
                        plots=plots,
                        callbacks=callbacks,
                        compute_loss=compute_loss,
                    )  # val best model with plots
                    if is_coco:
                        callbacks.run("on_fit_epoch_end", list(mloss) + list(results) + lr, epoch, best_fitness, fi)

        callbacks.run("on_train_end", last, best, epoch, results)

    torch.cuda.empty_cache()
    return results


def parse_opt(known=False):
    """
    Parse command-line arguments for YOLOv5 training, validation, and testing.

    Args:
        known (bool, optional): If True, parses known arguments, ignoring the unknown. Defaults to False.

    Returns:
        (argparse.Namespace): Parsed command-line arguments containing options for YOLOv5 execution.

    Example:
        ```python
        from ultralytics.yolo import parse_opt
        opt = parse_opt()
        print(opt)
        ```

    Links:
        - Models: https://github.com/ultralytics/yolov5/tree/master/models
        - Datasets: https://github.com/ultralytics/yolov5/tree/master/data
        - Tutorial: https://docs.ultralytics.com/yolov5/tutorials/train_custom_data
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("--weights", type=str, default=ROOT / "yolov5s.pt", help="initial weights path")
    parser.add_argument("--cfg", type=str, default="", help="model.yaml path")
    parser.add_argument("--data", type=str, default=ROOT / "data/coco128.yaml", help="dataset.yaml path")
    parser.add_argument("--hyp", type=str, default=ROOT / "data/hyps/hyp.scratch-low.yaml", help="hyperparameters path")
    parser.add_argument("--epochs", type=int, default=100, help="total training epochs")
    parser.add_argument("--batch-size", type=int, default=16, help="total batch size for all GPUs, -1 for autobatch")
    parser.add_argument("--imgsz", "--img", "--img-size", type=int, default=640, help="train, val image size (pixels)")
    parser.add_argument("--rect", action="store_true", help="rectangular training")
    parser.add_argument("--resume", nargs="?", const=True, default=False, help="resume most recent training")
    parser.add_argument("--nosave", action="store_true", help="only save final checkpoint")
    parser.add_argument("--noval", action="store_true", help="only validate final epoch")
    parser.add_argument("--noautoanchor", action="store_true", help="disable AutoAnchor")
    parser.add_argument("--noplots", action="store_true", help="save no plot files")
    parser.add_argument("--evolve", type=int, nargs="?", const=300, help="evolve hyperparameters for x generations")
    parser.add_argument(
        "--evolve_population", type=str, default=ROOT / "data/hyps", help="location for loading population"
    )
    parser.add_argument("--resume_evolve", type=str, default=None, help="resume evolve from last generation")
    parser.add_argument("--bucket", type=str, default="", help="gsutil bucket")
    parser.add_argument("--cache", type=str, nargs="?", const="ram", help="image --cache ram/disk")
    parser.add_argument("--image-weights", action="store_true", help="use weighted image selection for training")
    parser.add_argument("--device", default="", help="cuda device, i.e. 0 or 0,1,2,3 or cpu")
    parser.add_argument("--multi-scale", action="store_true", help="vary img-size +/- 50%%")
    parser.add_argument("--single-cls", action="store_true", help="train multi-class data as single-class")
    parser.add_argument("--optimizer", type=str, choices=["SGD", "Adam", "AdamW"], default="SGD", help="optimizer")
    parser.add_argument("--sync-bn", action="store_true", help="use SyncBatchNorm, only available in DDP mode")
    parser.add_argument("--workers", type=int, default=8, help="max dataloader workers (per RANK in DDP mode)")
    parser.add_argument("--project", default=ROOT / "runs/train", help="save to project/name")
    parser.add_argument("--name", default="exp", help="save to project/name")
    parser.add_argument("--exist-ok", action="store_true", help="existing project/name ok, do not increment")
    parser.add_argument("--quad", action="store_true", help="quad dataloader")
    parser.add_argument("--cos-lr", action="store_true", help="cosine LR scheduler")
    parser.add_argument("--label-smoothing", type=float, default=0.0, help="Label smoothing epsilon")
    parser.add_argument("--patience", type=int, default=100, help="EarlyStopping patience (epochs without improvement)")
    parser.add_argument("--freeze", nargs="+", type=int, default=[0], help="Freeze layers: backbone=10, first3=0 1 2")
    parser.add_argument("--save-period", type=int, default=-1, help="Save checkpoint every x epochs (disabled if < 1)")
    parser.add_argument("--seed", type=int, default=0, help="Global training seed")
    parser.add_argument("--local_rank", type=int, default=-1, help="Automatic DDP Multi-GPU argument, do not modify")

    # Logger arguments
    parser.add_argument("--entity", default=None, help="Entity")
    parser.add_argument("--upload_dataset", nargs="?", const=True, default=False, help='Upload data, "val" option')
    parser.add_argument("--bbox_interval", type=int, default=-1, help="Set bounding-box image logging interval")
    parser.add_argument("--artifact_alias", type=str, default="latest", help="Version of dataset artifact to use")

    # NDJSON logging
    parser.add_argument("--ndjson-console", action="store_true", help="Log ndjson to console")
    parser.add_argument("--ndjson-file", action="store_true", help="Log ndjson to file")

    return parser.parse_known_args()[0] if known else parser.parse_args()


def main(opt, callbacks=Callbacks()):
    """
    Runs the main entry point for training or hyperparameter evolution with specified options and optional callbacks.

    Args:
        opt (argparse.Namespace): The command-line arguments parsed for YOLOv5 training and evolution.
        callbacks (ultralytics.utils.callbacks.Callbacks, optional): Callback functions for various training stages.
            Defaults to Callbacks().

    Returns:
        None

    Note:
        For detailed usage, refer to:
        https://github.com/ultralytics/yolov5/tree/master/models
    """
    if RANK in {-1, 0}:
        print_args(vars(opt))
        check_git_status()
        check_requirements(ROOT / "requirements.txt")

    # Resume (from specified or most recent last.pt)
    if opt.resume and not check_comet_resume(opt) and not opt.evolve:
        last = Path(check_file(opt.resume) if isinstance(opt.resume, str) else get_latest_run())
        opt_yaml = last.parent.parent / "opt.yaml"  # train options yaml
        opt_data = opt.data  # original dataset
        if opt_yaml.is_file():
            with open(opt_yaml, errors="ignore") as f:
                d = yaml.safe_load(f)
        else:
            d = torch.load(last, map_location="cpu")["opt"]
        opt = argparse.Namespace(**d)  # replace
        opt.cfg, opt.weights, opt.resume = "", str(last), True  # reinstate
        if is_url(opt_data):
            opt.data = check_file(opt_data)  # avoid HUB resume auth timeout
    else:
        opt.data, opt.cfg, opt.hyp, opt.weights, opt.project = (
            check_file(opt.data),
            check_yaml(opt.cfg),
            check_yaml(opt.hyp),
            str(opt.weights),
            str(opt.project),
        )  # checks
        assert len(opt.cfg) or len(opt.weights), "either --cfg or --weights must be specified"
        if opt.evolve:
            if opt.project == str(ROOT / "runs/train"):  # if default project name, rename to runs/evolve
                opt.project = str(ROOT / "runs/evolve")
            opt.exist_ok, opt.resume = opt.resume, False  # pass resume to exist_ok and disable resume
        if opt.name == "cfg":
            opt.name = Path(opt.cfg).stem  # use model.yaml as name
        opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))

    # DDP mode
    device = select_device(opt.device, batch_size=opt.batch_size)
    if LOCAL_RANK != -1:
        msg = "is not compatible with YOLOv5 Multi-GPU DDP training"
        assert not opt.image_weights, f"--image-weights {msg}"
        assert not opt.evolve, f"--evolve {msg}"
        assert opt.batch_size != -1, f"AutoBatch with --batch-size -1 {msg}, please pass a valid --batch-size"
        assert opt.batch_size % WORLD_SIZE == 0, f"--batch-size {opt.batch_size} must be multiple of WORLD_SIZE"
        assert torch.cuda.device_count() > LOCAL_RANK, "insufficient CUDA devices for DDP command"
        torch.cuda.set_device(LOCAL_RANK)
        device = torch.device("cuda", LOCAL_RANK)
        dist.init_process_group(
            backend="nccl" if dist.is_nccl_available() else "gloo", timeout=timedelta(seconds=10800)
        )

    # Train
    if not opt.evolve:
        train(opt.hyp, opt, device, callbacks)

    # Evolve hyperparameters (optional)
    else:
        # Hyperparameter evolution metadata (including this hyperparameter True-False, lower_limit, upper_limit)
        meta = {
            "lr0": (False, 1e-5, 1e-1),  # initial learning rate (SGD=1E-2, Adam=1E-3)
            "lrf": (False, 0.01, 1.0),  # final OneCycleLR learning rate (lr0 * lrf)
            "momentum": (False, 0.6, 0.98),  # SGD momentum/Adam beta1
            "weight_decay": (False, 0.0, 0.001),  # optimizer weight decay
            "warmup_epochs": (False, 0.0, 5.0),  # warmup epochs (fractions ok)
            "warmup_momentum": (False, 0.0, 0.95),  # warmup initial momentum
            "warmup_bias_lr": (False, 0.0, 0.2),  # warmup initial bias lr
            "box": (False, 0.02, 0.2),  # box loss gain
            "cls": (False, 0.2, 4.0),  # cls loss gain
            "cls_pw": (False, 0.5, 2.0),  # cls BCELoss positive_weight
            "obj": (False, 0.2, 4.0),  # obj loss gain (scale with pixels)
            "obj_pw": (False, 0.5, 2.0),  # obj BCELoss positive_weight
            "iou_t": (False, 0.1, 0.7),  # IoU training threshold
            "anchor_t": (False, 2.0, 8.0),  # anchor-multiple threshold
            "anchors": (False, 2.0, 10.0),  # anchors per output grid (0 to ignore)
            "fl_gamma": (False, 0.0, 2.0),  # focal loss gamma (efficientDet default gamma=1.5)
            "hsv_h": (True, 0.0, 0.1),  # image HSV-Hue augmentation (fraction)
            "hsv_s": (True, 0.0, 0.9),  # image HSV-Saturation augmentation (fraction)
            "hsv_v": (True, 0.0, 0.9),  # image HSV-Value augmentation (fraction)
            "degrees": (True, 0.0, 45.0),  # image rotation (+/- deg)
            "translate": (True, 0.0, 0.9),  # image translation (+/- fraction)
            "scale": (True, 0.0, 0.9),  # image scale (+/- gain)
            "shear": (True, 0.0, 10.0),  # image shear (+/- deg)
            "perspective": (True, 0.0, 0.001),  # image perspective (+/- fraction), range 0-0.001
            "flipud": (True, 0.0, 1.0),  # image flip up-down (probability)
            "fliplr": (True, 0.0, 1.0),  # image flip left-right (probability)
            "mosaic": (True, 0.0, 1.0),  # image mosaic (probability)
            "mixup": (True, 0.0, 1.0),  # image mixup (probability)
            "copy_paste": (True, 0.0, 1.0),  # segment copy-paste (probability)
        }

        # GA configs
        pop_size = 50
        mutation_rate_min = 0.01
        mutation_rate_max = 0.5
        crossover_rate_min = 0.5
        crossover_rate_max = 1
        min_elite_size = 2
        max_elite_size = 5
        tournament_size_min = 2
        tournament_size_max = 10

        with open(opt.hyp, errors="ignore") as f:
            hyp = yaml.safe_load(f)  # load hyps dict
            if "anchors" not in hyp:  # anchors commented in hyp.yaml
                hyp["anchors"] = 3
        if opt.noautoanchor:
            del hyp["anchors"], meta["anchors"]
        opt.noval, opt.nosave, save_dir = True, True, Path(opt.save_dir)  # only val/save final epoch
        # ei = [isinstance(x, (int, float)) for x in hyp.values()]  # evolvable indices
        evolve_yaml, evolve_csv = save_dir / "hyp_evolve.yaml", save_dir / "evolve.csv"
        if opt.bucket:
            # download evolve.csv if exists
            subprocess.run(
                [
                    "gsutil",
                    "cp",
                    f"gs://{opt.bucket}/evolve.csv",
                    str(evolve_csv),
                ]
            )

        # Delete the items in meta dictionary whose first value is False
        del_ = [item for item, value_ in meta.items() if value_[0] is False]
        hyp_GA = hyp.copy()  # Make a copy of hyp dictionary
        for item in del_:
            del meta[item]  # Remove the item from meta dictionary
            del hyp_GA[item]  # Remove the item from hyp_GA dictionary

        # Set lower_limit and upper_limit arrays to hold the search space boundaries
        lower_limit = np.array([meta[k][1] for k in hyp_GA.keys()])
        upper_limit = np.array([meta[k][2] for k in hyp_GA.keys()])

        # Create gene_ranges list to hold the range of values for each gene in the population
        gene_ranges = [(lower_limit[i], upper_limit[i]) for i in range(len(upper_limit))]

        # Initialize the population with initial_values or random values
        initial_values = []

        # If resuming evolution from a previous checkpoint
        if opt.resume_evolve is not None:
            assert os.path.isfile(ROOT / opt.resume_evolve), "evolve population path is wrong!"
            with open(ROOT / opt.resume_evolve, errors="ignore") as f:
                evolve_population = yaml.safe_load(f)
                for value in evolve_population.values():
                    value = np.array([value[k] for k in hyp_GA.keys()])
                    initial_values.append(list(value))

        # If not resuming from a previous checkpoint, generate initial values from .yaml files in opt.evolve_population
        else:
            yaml_files = [f for f in os.listdir(opt.evolve_population) if f.endswith(".yaml")]
            for file_name in yaml_files:
                with open(os.path.join(opt.evolve_population, file_name)) as yaml_file:
                    value = yaml.safe_load(yaml_file)
                    value = np.array([value[k] for k in hyp_GA.keys()])
                    initial_values.append(list(value))

        # Generate random values within the search space for the rest of the population
        if initial_values is None:
            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size)]
        elif pop_size > 1:
            population = [generate_individual(gene_ranges, len(hyp_GA)) for _ in range(pop_size - len(initial_values))]
            for initial_value in initial_values:
                population = [initial_value] + population

        # Run the genetic algorithm for a fixed number of generations
        list_keys = list(hyp_GA.keys())
        for generation in range(opt.evolve):
            if generation >= 1:
                save_dict = {}
                for i in range(len(population)):
                    little_dict = {list_keys[j]: float(population[i][j]) for j in range(len(population[i]))}
                    save_dict[f"gen{str(generation)}number{str(i)}"] = little_dict

                with open(save_dir / "evolve_population.yaml", "w") as outfile:
                    yaml.dump(save_dict, outfile, default_flow_style=False)

            # Adaptive elite size
            elite_size = min_elite_size + int((max_elite_size - min_elite_size) * (generation / opt.evolve))
            # Evaluate the fitness of each individual in the population
            fitness_scores = []
            for individual in population:
                for key, value in zip(hyp_GA.keys(), individual):
                    hyp_GA[key] = value
                hyp.update(hyp_GA)
                results = train(hyp.copy(), opt, device, callbacks)
                callbacks = Callbacks()
                # Write mutation results
                keys = (
                    "metrics/precision",
                    "metrics/recall",
                    "metrics/mAP_0.5",
                    "metrics/mAP_0.5:0.95",
                    "val/box_loss",
                    "val/obj_loss",
                    "val/cls_loss",
                )
                print_mutation(keys, results, hyp.copy(), save_dir, opt.bucket)
                fitness_scores.append(results[2])

            # Select the fittest individuals for reproduction using adaptive tournament selection
            selected_indices = []
            for _ in range(pop_size - elite_size):
                # Adaptive tournament size
                tournament_size = max(
                    max(2, tournament_size_min),
                    int(min(tournament_size_max, pop_size) - (generation / (opt.evolve / 10))),
                )
                # Perform tournament selection to choose the best individual
                tournament_indices = random.sample(range(pop_size), tournament_size)
                tournament_fitness = [fitness_scores[j] for j in tournament_indices]
                winner_index = tournament_indices[tournament_fitness.index(max(tournament_fitness))]
                selected_indices.append(winner_index)

            # Add the elite individuals to the selected indices
            elite_indices = [i for i in range(pop_size) if fitness_scores[i] in sorted(fitness_scores)[-elite_size:]]
            selected_indices.extend(elite_indices)
            # Create the next generation through crossover and mutation
            next_generation = []
            for _ in range(pop_size):
                parent1_index = selected_indices[random.randint(0, pop_size - 1)]
                parent2_index = selected_indices[random.randint(0, pop_size - 1)]
                # Adaptive crossover rate
                crossover_rate = max(
                    crossover_rate_min, min(crossover_rate_max, crossover_rate_max - (generation / opt.evolve))
                )
                if random.uniform(0, 1) < crossover_rate:
                    crossover_point = random.randint(1, len(hyp_GA) - 1)
                    child = population[parent1_index][:crossover_point] + population[parent2_index][crossover_point:]
                else:
                    child = population[parent1_index]
                # Adaptive mutation rate
                mutation_rate = max(
                    mutation_rate_min, min(mutation_rate_max, mutation_rate_max - (generation / opt.evolve))
                )
                for j in range(len(hyp_GA)):
                    if random.uniform(0, 1) < mutation_rate:
                        child[j] += random.uniform(-0.1, 0.1)
                        child[j] = min(max(child[j], gene_ranges[j][0]), gene_ranges[j][1])
                next_generation.append(child)
            # Replace the old population with the new generation
            population = next_generation
        # Print the best solution found
        best_index = fitness_scores.index(max(fitness_scores))
        best_individual = population[best_index]
        print("Best solution found:", best_individual)
        # Plot results
        plot_evolve(evolve_csv)
        LOGGER.info(
            f'Hyperparameter evolution finished {opt.evolve} generations\n'
            f"Results saved to {colorstr('bold', save_dir)}\n"
            f'Usage example: $ python train.py --hyp {evolve_yaml}'
        )


def generate_individual(input_ranges, individual_length):
    """
    Generate an individual with random hyperparameters within specified ranges.

    Args:
        input_ranges (list[tuple[float, float]]): List of tuples where each tuple contains the lower and upper bounds
            for the corresponding gene (hyperparameter).
        individual_length (int): The number of genes (hyperparameters) in the individual.

    Returns:
        list[float]: A list representing a generated individual with random gene values within the specified ranges.

    Example:
        ```python
        input_ranges = [(0.01, 0.1), (0.1, 1.0), (0.9, 2.0)]
        individual_length = 3
        individual = generate_individual(input_ranges, individual_length)
        print(individual)  # Output: [0.035, 0.678, 1.456] (example output)
        ```

    Note:
        The individual returned will have a length equal to `individual_length`, with each gene value being a floating-point
        number within its specified range in `input_ranges`.
    """
    individual = []
    for i in range(individual_length):
        lower_bound, upper_bound = input_ranges[i]
        individual.append(random.uniform(lower_bound, upper_bound))
    return individual


def run(**kwargs):
    """
    Execute YOLOv5 training with specified options, allowing optional overrides through keyword arguments.

    Args:
        weights (str, optional): Path to initial weights. Defaults to ROOT / 'yolov5s.pt'.
        cfg (str, optional): Path to model YAML configuration. Defaults to an empty string.
        data (str, optional): Path to dataset YAML configuration. Defaults to ROOT / 'data/coco128.yaml'.
        hyp (str, optional): Path to hyperparameters YAML configuration. Defaults to ROOT / 'data/hyps/hyp.scratch-low.yaml'.
        epochs (int, optional): Total number of training epochs. Defaults to 100.
        batch_size (int, optional): Total batch size for all GPUs. Use -1 for automatic batch size determination. Defaults to 16.
        imgsz (int, optional): Image size (pixels) for training and validation. Defaults to 640.
        rect (bool, optional): Use rectangular training. Defaults to False.
        resume (bool | str, optional): Resume most recent training with an optional path. Defaults to False.
        nosave (bool, optional): Only save the final checkpoint. Defaults to False.
        noval (bool, optional): Only validate at the final epoch. Defaults to False.
        noautoanchor (bool, optional): Disable AutoAnchor. Defaults to False.
        noplots (bool, optional): Do not save plot files. Defaults to False.
        evolve (int, optional): Evolve hyperparameters for a specified number of generations. Use 300 if provided without a
            value.
        evolve_population (str, optional): Directory for loading population during evolution. Defaults to ROOT / 'data/ hyps'.
        resume_evolve (str, optional): Resume hyperparameter evolution from the last generation. Defaults to None.
        bucket (str, optional): gsutil bucket for saving checkpoints. Defaults to an empty string.
        cache (str, optional): Cache image data in 'ram' or 'disk'. Defaults to None.
        image_weights (bool, optional): Use weighted image selection for training. Defaults to False.
        device (str, optional): CUDA device identifier, e.g., '0', '0,1,2,3', or 'cpu'. Defaults to an empty string.
        multi_scale (bool, optional): Use multi-scale training, varying image size by Â±50%. Defaults to False.
        single_cls (bool, optional): Train with multi-class data as single-class. Defaults to False.
        optimizer (str, optional): Optimizer type, choices are ['SGD', 'Adam', 'AdamW']. Defaults to 'SGD'.
        sync_bn (bool, optional): Use synchronized BatchNorm, only available in DDP mode. Defaults to False.
        workers (int, optional): Maximum dataloader workers per rank in DDP mode. Defaults to 8.
        project (str, optional): Directory for saving training runs. Defaults to ROOT / 'runs/train'.
        name (str, optional): Name for saving the training run. Defaults to 'exp'.
        exist_ok (bool, optional): Allow existing project/name without incrementing. Defaults to False.
        quad (bool, optional): Use quad dataloader. Defaults to False.
        cos_lr (bool, optional): Use cosine learning rate scheduler. Defaults to False.
        label_smoothing (float, optional): Label smoothing epsilon value. Defaults to 0.0.
        patience (int, optional): Patience for early stopping, measured in epochs without improvement. Defaults to 100.
        freeze (list, optional): Layers to freeze, e.g., backbone=10, first 3 layers = [0, 1, 2]. Defaults to [0].
        save_period (int, optional): Frequency in epochs to save checkpoints. Disabled if < 1. Defaults to -1.
        seed (int, optional): Global training random seed. Defaults to 0.
        local_rank (int, optional): Automatic DDP Multi-GPU argument. Do not modify. Defaults to -1.

    Returns:
        None: The function initiates YOLOv5 training or hyperparameter evolution based on the provided options.

    Examples:
        ```python
        import train
        train.run(data='coco128.yaml', imgsz=320, weights='yolov5m.pt')
        ```

    Notes:
        - Models: https://github.com/ultralytics/yolov5/tree/master/models
        - Datasets: https://github.com/ultralytics/yolov5/tree/master/data
        - Tutorial: https://docs.ultralytics.com/yolov5/tutorials/train_custom_data
    """
    opt = parse_opt(True)
    for k, v in kwargs.items():
        setattr(opt, k, v)
    main(opt)
    return opt


if __name__ == "__main__":
    opt = parse_opt()
    main(opt)
